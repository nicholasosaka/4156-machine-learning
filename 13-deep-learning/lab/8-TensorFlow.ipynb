{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\Wm}{\\mathbf{W}}\n",
    " \\newcommand{\\Ym}{\\mathbf{Y}}\n",
    " \\newcommand{\\I}{\\mathbf{I}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "# Tensorflow 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is Tensorflow\n",
    "TensorFlow is a  numerical computation library that assists users with building, training, and testing machine learning models. TensorFlow got its name because the inputs into models are tensors which then flow through a list of operations. This \"list\" of operations can be visualized as a graph or a flowchart.\n",
    "\n",
    "## Computational Graphs\n",
    "TensorFlow uses a graph framework as eluded to before. Each graph constructed by TensorFlow is made up of operations referred to as **op**. Each node in the graph is an op called an **op node**. Each op node is connected to other op nodes which then forms a graph. The graph can be thought of as an outline for the path an input Tensor will follow. As a general note, an op takes tensors as input and produces tensors as output.\n",
    "\n",
    "\n",
    "Internally, a kind of just-in-time (JIT) compiler extracts the computation graph(s) from the Python code. The extracted graph is optimized by pruning unused nodes, and then runs efficiently by automatically running independent operations in parallel.  \n",
    "\n",
    "In general, TensorFlow uses graphs to define the training and testing computations needed to build a model. TensorFlow even has a neat library called TensorBoard that let's visualize these graphs!\n",
    "\n",
    "![](https://external-content.duckduckgo.com/iu/?u=http%3A%2F%2Feasy-tensorflow.com%2Ffiles%2F2_6.png&f=1&nofb=1)\n",
    "\n",
    "\n",
    "## What is a Tensor?\n",
    "A tensor can be conceptualized as a generalization for matrix, vector, or a scalar. This means a tensor can take the form of a scalar, 1-D matrix, or any N-D matrix. For now, it is okay to conceptualize a matrix and tensor as the same idea. However, outside machine and deep learning this isn't true. There is more nuance in the differences between matrices and tensors but the aforementioned definition should suffice for now.\n",
    "\n",
    "> Tensor: In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor.\n",
    "> <br> -- p.31 *Deep Learning* by Ian Goodfellow\n",
    "\n",
    "![](https://miro.medium.com/max/449/1*kHues3bfBOythrXNLosesQ.png)\n",
    "\n",
    "### Tensor Rank\n",
    "All tensors have a rank which describes the total number of dimensions. For instance, if we had a tensor with the shape (3, 3) then we would say this tensor has a rank of 2, since there are 2 dimensions.\n",
    " \n",
    "> rank 0: 3 <br/>\n",
    "> rank 1: [1,2,3] <br/>\n",
    "> rank 2: [[1,2],[2,3]] <br/>\n",
    "> rank 3: [[[1,2,3],[2,3,4]],[[1,1,1],[2,2,2]]] <br/>\n",
    "> ...\n",
    " \n",
    "## Extra Material\n",
    "### Readings\n",
    "- [What’s the difference between a matrix and a tensor?](https://medium.com/@quantumsteinke/whats-the-difference-between-a-matrix-and-a-tensor-4505fbdc576c)\n",
    "- [Tensors Illustrated](https://hackernoon.com/learning-ai-if-you-suck-at-math-p4-tensors-illustrated-with-cats-27f0002c9b32)\n",
    "- [GeeksforGeeks - TensorFlow 2.0](https://www.geeksforgeeks.org/tensorflow-2-0/)\n",
    "\n",
    "### Videos\n",
    "- [Tensors Explained - Data Structures of Deep Learning](https://www.youtube.com/watch?v=Csa5R12jYRg)\n",
    "- [What's a Tensor (technical)](https://www.youtube.com/watch?v=f5liqUk0ZTw)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Installation\n",
    "\n",
    "Make sure install TensorFlow before starting this lab. You can follow the instructions from \n",
    "https://www.tensorflow.org/install/. Once you have finished installing, you can import TensorFlow as follows. Assertion here checks if your installation is TensorFlow v2.0 or above. If you are failing the assertion here then your TensroFlow version is most likely below v2.0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "assert tf.__version__ >= \"2.0\" # check if your ensorflow version is above 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Tools from text for consistant results\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Exploring Tensors\n",
    "Let's start off by exploring how to create some simple tensors. You'll come to notice that there are some parallels between TensorFlow's tensors syntax and Numpy's array syntax. This is on purpose and we'll see more compatibilities between tensors and arrays later. \n",
    "\n",
    "First, let's start off by defining a scalar tensor constant called `node1`. A `tf.constant` means exactly what you might expect, we can not change the value of this constant once defined. If we want to define a tensor that can be changed we use `tf.Variable`. Notice, we can define the data type for the constant using the 'dtype' parameter. [Here](https://www.tensorflow.org/api_docs/python/tf/dtypes/DType) is the documentation on all the different data types TensorFlow supports.\n",
    "\n",
    "If you're curious about `tf.constant` or `tf.Variable` then uncomment the following lines to see their documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?tf.Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: tf.Tensor(3.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node1 = tf.constant(3.0, dtype=tf.float32)\n",
    "print('node1:', node1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we print our `node1` variable TensorFlow automatically prints some useful information. This being the data, shape, and type. **Note, you might get slightly different output if you let Jupyter print instead of using Python's `print()` function.**\n",
    "\n",
    "Let's define our second scalar constant `node2`. Notice that TensorFlow will automatically assign types. In this case, it will automatically assign float32 to our `node2` tensor because the numbers we initialized with had decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node2: tf.Tensor(4.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "node2 = tf.constant(4.0) \n",
    "print('node2:', node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, instead of defining constant scalars we can define two constant vectors. Here, our `nodeb` defaults to a data type of int32 because no decimals were used when initializing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodea: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "nodeb: tf.Tensor([4 5 6], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "nodea = tf.constant([1,2,3], dtype=tf.float32)\n",
    "nodeb = tf.constant([4,5,6]) # defaults to tf.int32\n",
    "print('nodea:', nodea)\n",
    "print('nodeb:', nodeb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's define two 2-D constant tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodeA: tf.Tensor(\n",
      "[[1. 2. 3.]\n",
      " [1. 2. 3.]], shape=(2, 3), dtype=float32)\n",
      "nodeB: tf.Tensor(\n",
      "[[4 5 6]\n",
      " [4 5 6]], shape=(2, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "nodeA = tf.constant([[1,2,3], [1,2,3]], dtype=tf.float32)\n",
    "nodeB = tf.constant([[4,5,6], [4,5,6]]) # defaults to tf.int32\n",
    "print('nodeA:', nodeA)\n",
    "print('nodeB:', nodeB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare all our shapes and data types. Just like in Numpy, we can check our the shape our tensor by using the `.shape()` method. Likewise, we can check the data type by using the `.dtype()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1 shape: ()\n",
      "nodeb shape: (3,)\n",
      "nodeB shape: (2, 3)\n"
     ]
    }
   ],
   "source": [
    "print('node1 shape:', node1.shape)\n",
    "print('nodeb shape:', nodeb.shape)\n",
    "print('nodeB shape:', nodeB.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1 shape: <dtype: 'float32'>\n",
      "nodeb shape: <dtype: 'int32'>\n",
      "nodeB shape: <dtype: 'int32'>\n"
     ]
    }
   ],
   "source": [
    "print('node1 shape:', node1.dtype)\n",
    "print('nodeb shape:', nodeb.dtype)\n",
    "print('nodeB shape:', nodeB.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want to convert our tensor to a NumPy array we simply call the `.numpy()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1: 3.0\n",
      "nodeb: [4 5 6]\n",
      "nodeB:\n",
      " [[4 5 6]\n",
      " [4 5 6]]\n"
     ]
    }
   ],
   "source": [
    "print(\"node1:\", node1.numpy())\n",
    "print(\"nodeb:\", nodeb.numpy()) \n",
    "print(\"nodeB:\\n\", nodeB.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Creating a Graph\n",
    "\n",
    "Now that we know how to define tensors, let's compute a simple graph by adding two of our previously defined tensors together! Below is an example of what the graph might look like. Here we can imagine the const3 and const4 nodes representing two of our previously defined tensors. Both the const3 and const4 nodes point to the add node indicating an add operation is applied.\n",
    "\n",
    "![](https://www.tensorflow.org/images/getting_started_add.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first start off by adding `node1` and `node2` together. Notice, that both of these tensors are of the same type. Meaning, both `node1` and `node2` have a data type of float32!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(node1 + node2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see our two nodes were added together and our tensor now has the value of 7!\n",
    "\n",
    "What happens if we try adding tensors of different types? Let's see by wrapping our addition operation of our 1-D tensors `nodea` and `nodeb` in a try except."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cannot compute AddV2 as input #1(zero-based) was expected to be a float tensor but is a int32 tensor [Op:AddV2] name: add/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    nodea + nodeb\n",
    "except tf.errors.InvalidArgumentError as ex:\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we caught an error that indicates to us that we can't add two tensors of different data types. To fix this we can cast `nodea` as an int32 by using the `tf.cast()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodea before casting: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "nodea after casting: tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('nodea before casting:', nodea)\n",
    "print('nodea after casting:', tf.cast(nodea, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when we try adding our casted `nodea` our addition operation works flawlessly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([5 7 9], shape=(3,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.cast(nodea, tf.int32) + nodeb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add our two 2-D tensors together. Recall, that `nodeB` is an int32 so we must cast it as a float32 or we must cast `nodeA` as an int32. Let's try adding `nodeA` and `nodeB` while casting `nodeB` as a float32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[5. 7. 9.]\n",
      " [5. 7. 9.]], shape=(2, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "nodeC = nodeA + tf.cast(nodeB, tf.float32)\n",
    "print(nodeC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing\n",
    "\n",
    "Just like with Numpy arrays we can slice and index tensors. This done exactly like we are used to doing with Numpy. Let's slice the second column out of `nodeC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([7., 7.], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodeC[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "Notice our shape when slicing out the second column is (2,)! As most of you may know by now, working with a single dimension vector can produce weird results later on down the line. Let's splice the second column from `nodeC` tensor, but this time lets make sure it's a matrix with the shape (2, 1).\n",
    "\n",
    "1. Try slicing second column of `nodeC` so that the resulting output is a (2, 1) matrix. \n",
    "    1. Try using `tf.reshape()`, `tf.expand_dims()`, or slicing the array and adding `tf.newaxis` or `None` to the 3rd dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO (1)\n",
    "column_matrix = \n",
    "if (2,1) != column_matrix.shape:\n",
    "    raise ValueError(\"Your sliced matrix does not have the shape (2, 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NumPy and TensorFlow\n",
    "TensorFlow went through great lengths to allow for relatively seamless transitions between NumPy arrays and TensorFlow tensors. Let's take a closer look at some of these transitions. We'll start off by defining a NumPy array `array`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.arange(4)\n",
    "array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert `array` to TensorFlow tensor `tensor`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 2, 3])>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = tf.constant(array)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we convert it back to NumPy array? We can by calling `.numpy()` as mentioned before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also can simply cast our tensor using the `np.array()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out TensorFlow and NumPy functions will **typically** automatically account for the conversion between tensors and arrays for you. For instance, we can feed NumPy arrays to TensorFlow functions. Likewise, we can feed tensors to NumPy functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we pass a NumPy array to the TenorFlow `tf.square()` function. As we can see the function works fine which returns a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=int64, numpy=array([0, 1, 4, 9])>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(array)  # passing numpy array to tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, here we pass a tensor to the NumPy `np.square()` function which returns a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 4, 9])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.square(tensor)  # passing tf tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Primitives \n",
    "Other than NumPy arrays TensorFlow supports several other primitives. We'll quickly go through some of other primitive types tensors support in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strings\n",
    "\n",
    "Tensors can represent more than just numerical values. Here we can set a tensor equal to a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "1. Create a string tensor by passing the string \"Hello, world!\" to `tf.constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello, world!', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "# TODO (1)\n",
    "string_tensor = \n",
    "print(string_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 72 101 108 108 111  44  32 119 111 114 108 100  33], shape=(13,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Convert string to unicode\n",
    "string = \"Hello, world!\"\n",
    "[ord(ch) for ch in string]\n",
    "\n",
    "# Load unicode into tensor\n",
    "unicode_tensor = tf.strings.unicode_decode(string, \"UTF-8\")\n",
    "print(unicode_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(b'Hello, world!', shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "string_tensor = tf.strings.unicode_encode(unicode_tensor, \"UTF-8\")\n",
    "print(string_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ragged Tensors\n",
    "\n",
    "A ragged tensor is a kind of tensors allows the elements to have variable lengths. Let's take a look at this idea using string tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'hello' b'world!' b'itcs4156'], shape=(3,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "string_tensor = tf.constant(['hello', 'world!', 'itcs4156'])\n",
    "print(string_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we convert the string elements into unicode, it will be a ragged tensor as it has variable length integer arrays.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 6, 8], dtype=int32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.length(string_tensor, unit=\"UTF8_CHAR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[104, 101, 108, 108, 111], [119, 111, 114, 108, 100, 33], [105, 116, 99, 115, 52, 49, 53, 54]]>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragged_tensor = tf.strings.unicode_decode(string_tensor, \"UTF-8\")\n",
    "ragged_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the shape of our ragged tensor is (3, None). The 3 indicates that we have 3 elements and the 'None' indicates each element can have variable length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, None])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ragged_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "1. Now create a RaggedTensor by passing the following lists `[1,2,3]`, `[1]`, and `[1,2,3,4,5]`  to `tf.ragged.constant()`.\n",
    "    1. Hint: Be sure to wrap the all the aforementioned lists into one list before passing them to `tf.ragged.constant()`. In other words, create a list of lists as the input to `tf.ragged.constant()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.RaggedTensor [[1, 2, 3], [1], [1, 2, 3, 4, 5]]>\n",
      "(3, None)\n"
     ]
    }
   ],
   "source": [
    "# TODO (1)\n",
    "ragged_tensor = \n",
    "print(ragged_tensor)\n",
    "print(ragged_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if I need a fixed-sized tensor. You can use `.to_tensor()` to convert the ragged to the regular. Notice, that the 'None' becomes the size of the largest list in our tensor, i.e. 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2 3 0 0]\n",
      " [1 0 0 0 0]\n",
      " [1 2 3 4 5]], shape=(3, 5), dtype=int32)\n",
      "(3, 5)\n"
     ]
    }
   ],
   "source": [
    "tensor = ragged_tensor.to_tensor()\n",
    "print(tensor)\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Tensors\n",
    "\n",
    "Imagine you have a very large matrix but most of the elements are zeros. For instance, try to imagine a matrix with hundreds of dimensions and millions elements in each dimension, but only 2% of all the elements are actually nonzero. Storing a sparse matrix like this in memory and be extremely expensive. Luckily, TensorFlow has the `tf.SparseTensor` class that can save us a lot of memory.\n",
    "\n",
    "Below is a toy example of a sparse array where the nonzero elements are all 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_array = np.zeros((8, 8))\n",
    "sparse_array[(1, 1), (2, 5)] = 1\n",
    "sparse_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "1. Convert the sparse Numpy array `sparse_array` into a tensor using the `tf.constant()` function. Store output into `tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]], shape=(8, 8), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "# TODO (1)\n",
    "tensor = \n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert our sparse tensor into a special `SparseTensor` object, which will save us memory for large sparse tensors. To do so, we simply need to know the indices and the values for all the nonzero elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparseTensor(indices=tf.Tensor(\n",
      "[[1 2]\n",
      " [1 5]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 1], shape=(2,), dtype=int32), dense_shape=tf.Tensor([8 8], shape=(2,), dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "sparse_tensor = tf.SparseTensor(indices=[(1, 2), (1, 5)], values=[1,1], dense_shape=[8,8])\n",
    "print(sparse_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "For comparison, let's turn our `sparse_tensor` back into a regular tensor. \n",
    "\n",
    "1. Turn `sparse_tensor` back into a regular tensor by passing our `sparse_tensor` to `tf.sparse.to_dense()`. Store the output into `dense_tensor`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0]], shape=(8, 8), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# TODO (1)\n",
    "dense_tensor = \n",
    "print(dense_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Practice so far has been with `tf.constant`. Let's experiement using `tf.Variable` so that we can reassign the values in our tensor. This will be needed as our weights will need to be updated as we train our models. \n",
    "\n",
    "### TODO:\n",
    "\n",
    "1. Create a 1-D tensor using `tf.Variable` and pass `a` as the initial values. Store the output into `v`.\n",
    "2. Change the value 2 in our tensor `v` to be 5.\n",
    "    1. Determine the index position of 2 and then use the `.assign()` method on `v`, indexed at the position of 2, to then assign 5.\n",
    "3. Change all the values in `v` to be twice as large, i.e. v*2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 2, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "a = [1, 2, 3]\n",
    "# TODO (1)\n",
    "v = \n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([1, 5, 3], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# TODO (2)\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3,) dtype=int32, numpy=array([ 2, 10,  6], dtype=int32)>\n"
     ]
    }
   ],
   "source": [
    "# TODO (3)\n",
    "\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Linear Regression\n",
    "Now let's put TensorFlow into action! First let's start off by doing some simple linear regression using the famous California Housing dataset. The goal of this dataset is to predict housing prices. There is no need to download anything as we can load this dataset using `sklearn`.\n",
    "\n",
    "> The California Housing Prices data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
    "Recall the linear regression closed form formulas are as follows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the dataset which is a `sklearn` specific class called a `Bunch` object. Then we extract the data from the `Bunch` object to a NumPy array, and then finally cast our NumPy array as a Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load housing dataset Bunch object\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "# Concat and cast data as array\n",
    "X_T = np.c_[housing.data, housing.target]\n",
    "\n",
    "# Cast data as DataFrame\n",
    "df = pd.DataFrame(X_T)\n",
    "df.describe()\n",
    "\n",
    "# Add an extra feature name for target visualization\n",
    "df.columns = housing.feature_names + ['Avg. Value']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "For a quick data visualization we compute a correlation matrix using Pandas DataFrame `.corr()` method and then plot it using a heatmap. Here we can see negative and positive correlations between our data features based on the color of each element. For instance, we can see latitude and longitude have a strong negative correlation as the color bar indicates to us that the correlation value is around -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1dae6aeac8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition\n",
    "Do you remember the `partitionlib.py` Python file for data partitioning we a few weeks ago? Well, it's time to reuse it! Let's copy the `partitionlib.py` Python file into the directory where this notebook is running! If you don't have this file go to the [Files](https://uncc.instructure.com/courses/119993/files?preview=8006250) tab in Canvas to download the `partitionlib.py` Python file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import partitionlib as ptl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import partitionlib\n",
    "importlib.reload(partitionlib);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data(X, T):\n",
    "    \"\"\"Partition data into train and test splits.\"\"\"\n",
    "    data, targets = ptl.partition(X, T)\n",
    "    Xtrain, Xtest = data\n",
    "    Ttrain, Ttest = targets\n",
    "    return Xtrain, Xtest, Ttrain, Ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (16512, 8)\n",
      "Train target shape: (16512, 1)\n",
      "Test data shape: (4128, 8)\n",
      "Test target shape: (4128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Partition data\n",
    "Xtrain, Xtest, Ttrain, Ttest = partition_data(housing.data, housing.target)\n",
    "print(\"Train data shape: {}\".format(Xtrain.shape))\n",
    "print(\"Train target shape: {}\".format(Ttrain.shape))\n",
    "print(\"Test data shape: {}\".format(Xtest.shape))\n",
    "print(\"Test target shape: {}\".format(Ttest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of total features: 8\n",
      "Number of training samples: 16512\n",
      "Number of testing samples: 4128\n"
     ]
    }
   ],
   "source": [
    "# Save total number of features\n",
    "D = Xtrain.shape[1]\n",
    "# Save number of samples for train/trest\n",
    "Ntrain, Ntest = Xtrain.shape[0], Xtest.shape[0]\n",
    "print(\"Number of total features: {}\".format(D))\n",
    "print(\"Number of training samples: {}\".format(Ntrain))\n",
    "print(\"Number of testing samples: {}\".format(Ntest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "\n",
    "Next we can standardize our data using the`StandardScaler` class from `sklearn`. Don't forget, we have to readd our bias column to our newly standardized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# Let's normalize the data first with Scikit.learn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define standardization class and standardize our housing data\n",
    "scaler = StandardScaler()\n",
    "XtrainS = scaler.fit_transform(Xtrain)\n",
    "XtestS = scaler.fit_transform(Xtest)\n",
    "\n",
    "# Add bias to standardized data\n",
    "X1trainS = np.c_[np.ones((Ntrain, 1)), XtrainS]\n",
    "X1testS = np.c_[np.ones((Ntest, 1)), XtestS]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Least Squares\n",
    "Recall the following least squares closed form equation.\n",
    "$$\n",
    "\\Wm = (\\Xm^\\top \\Xm)^{-1} \\Xm^\\top \\Tm\n",
    "$$\n",
    "\n",
    "Below is the NumPy code that we would typically use to compute least squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Weights: [[ 2.07084113]\n",
      " [ 0.83862817]\n",
      " [ 0.11381035]\n",
      " [-0.27812104]\n",
      " [ 0.30519044]\n",
      " [-0.00217071]\n",
      " [-0.0450531 ]\n",
      " [-0.895343  ]\n",
      " [-0.87121145]]\n",
      "MSE: 0.5236257645685504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ben/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: FutureWarning: `rcond` parameter will change to the default of machine precision times ``max(M, N)`` where M and N are the input matrix dimensions.\n",
      "To use the future default and silence this warning we advise to pass `rcond=None`, to keep using the old, explicitly pass `rcond=-1`.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "T = housing.target.reshape(-1, 1)\n",
    "w = np.linalg.lstsq(X1trainS.T @ X1trainS, X1trainS.T @ Ttrain)[0]\n",
    "\n",
    "print('Learned Weights: {}'.format(w))\n",
    "print(\"MSE: {}\".format(np.mean((X1trainS @ w - Ttrain)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "Let's rewrite our Numpy code using TensorFlow.\n",
    "1. Cast our standardized training data with bias added `X1trainS` as a tensor by passing it to the `tf.constant()` function. Store the output inside  `Xt`.\n",
    "2. Cast our target `Ttrain` as a tensor passing it to the `tf.constant()` function. Store the output inside `Tt`.\n",
    "3. Using `tf.linalg.lstsq()` and `tf.matmul()` compute the weights. Store the output into `w`.\n",
    "    1. Hint: `tf.matmul()` replaces the dot product, i.e. the `@` symbol for NumPy arrays.\n",
    "    2. Hint: Use the closed form LS equation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16512, 9), dtype=float64, numpy=\n",
       "array([[ 1.00000000e+00, -2.44107037e-02, -1.40695934e+00, ...,\n",
       "        -3.34506573e-02,  1.11764313e+00, -3.66200446e-01],\n",
       "       [ 1.00000000e+00, -9.17555535e-01, -4.50649581e-01, ...,\n",
       "        -1.14682117e-01, -1.34822737e+00,  1.21952671e+00],\n",
       "       [ 1.00000000e+00,  6.62124076e-01,  1.07197775e-01, ...,\n",
       "        -1.77470305e-02,  1.11764313e+00, -8.79816474e-01],\n",
       "       ...,\n",
       "       [ 1.00000000e+00, -1.54850649e-02, -1.24757438e+00, ...,\n",
       "         1.53353197e-02,  9.25801215e-01, -6.80353939e-01],\n",
       "       [ 1.00000000e+00,  2.19063820e-01,  8.24430091e-01, ...,\n",
       "        -7.32401133e-02, -7.58664407e-01,  5.56313779e-01],\n",
       "       [ 1.00000000e+00, -3.68074202e-01, -9.28804458e-01, ...,\n",
       "         1.43409391e-04,  8.74331432e-01, -1.22887591e+00]])>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (1)\n",
    "Xt = \n",
    "Xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16512, 1), dtype=float64, numpy=\n",
       "array([[1.397],\n",
       "       [1.35 ],\n",
       "       [1.338],\n",
       "       ...,\n",
       "       [1.293],\n",
       "       [2.972],\n",
       "       [1.563]])>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2)\n",
    "Tt = \n",
    "Tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1), dtype=float64, numpy=\n",
       "array([[ 2.07084113],\n",
       "       [ 0.83862817],\n",
       "       [ 0.11381035],\n",
       "       [-0.27812104],\n",
       "       [ 0.30519044],\n",
       "       [-0.00217071],\n",
       "       [-0.0450531 ],\n",
       "       [-0.895343  ],\n",
       "       [-0.87121145]])>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (3)\n",
    "w = \n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Weights: [[ 2.07084113]\n",
      " [ 0.83862817]\n",
      " [ 0.11381035]\n",
      " [-0.27812104]\n",
      " [ 0.30519044]\n",
      " [-0.00217071]\n",
      " [-0.0450531 ]\n",
      " [-0.895343  ]\n",
      " [-0.87121145]]\n",
      "MSE: 0.5236257645685505\n"
     ]
    }
   ],
   "source": [
    "print('Learned Weights: {}'.format(w))\n",
    "print(\"MSE: {}\".format(np.mean((Xt @ w - Tt)**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Least Mean Squares and Gradient Descent \n",
    "\n",
    "Now, let's implement LMS with TensorFlow. Instead of computing each sample gradient one at a time, like normal, we'll compute all the gradients at once. To account for this, we must divide by our total number of training  samples $n$.\n",
    "\n",
    "Recall computing the gradient for a single sample (i.e. the derivative of our MSE loss function) is as follows.\n",
    "\n",
    "$\\nabla E_k = 2 *  x_k^T \\cdot( x_k \\cdot w - t_k)$\n",
    "\n",
    "Thus, if we want to compute all gradients at once our equation simply becomes the following.\n",
    "\n",
    "$\\nabla E = \\frac{2}{n} * X^T \\cdot (X \\cdot w  - T)$\n",
    "\n",
    "To compute our weight update recall we use the following formula.\n",
    "\n",
    "$w = w - \\alpha * \\nabla E$\n",
    "\n",
    "Here $\\alpha$ is our learning rate and $\\nabla E$ contains the gradients for for all the training data samples.\n",
    "\n",
    "### TODO:\n",
    "\n",
    "1. Compute the gradient for all our samples by taking the dot product between our data `Xt` and our error stored in `error`. Here `tf.matmul()` will replace of the dot product. In addition, use `tf.transpose()` to transpose any matrices.\n",
    "    1. Hint: Here $X$ corresponds to `Xt`, $T$ corresponds to `Tt`, $n$ corresponds to `Ntrain`, and $w$ corresponds to `w`. \n",
    "    1. Hint: Recall the the shape of `error` is (16512, 1) and the shape of `Xt` is (16512, 9).\n",
    "2. Using the weight update above, update our weights `w` using the tensor method `.assign()` instead of the equals.\n",
    "    1. Hint:  Here $w$ corresponds to `w`, $\\alpha$ corresponds to `learning_rate`, and $\\nabla E$ corresponds `gradients`   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initialized weights: <tf.Variable 'w:0' shape=(9, 1) dtype=float32, numpy=\n",
      "array([[ 0.9045429 ],\n",
      "       [ 0.35481548],\n",
      "       [ 0.5906365 ],\n",
      "       [ 0.51156354],\n",
      "       [-0.04808879],\n",
      "       [ 0.26202965],\n",
      "       [-0.62795925],\n",
      "       [-0.7713845 ],\n",
      "       [-0.32755637]], dtype=float32)>\n",
      "Epoch 0 MSE = 2.760789394378662\n",
      "Epoch 100 MSE = 0.6347358822822571\n",
      "Epoch 200 MSE = 0.5738407969474792\n",
      "Epoch 300 MSE = 0.5587007999420166\n",
      "Epoch 400 MSE = 0.5487741231918335\n",
      "Epoch 500 MSE = 0.5417068600654602\n",
      "Epoch 600 MSE = 0.5366416573524475\n",
      "Epoch 700 MSE = 0.533007025718689\n",
      "Epoch 800 MSE = 0.5303969383239746\n",
      "Epoch 900 MSE = 0.5285208225250244\n",
      "<tf.Variable 'w:0' shape=(9, 1) dtype=float32, numpy=\n",
      "array([[ 2.0708354 ],\n",
      "       [ 0.7820405 ],\n",
      "       [ 0.12636475],\n",
      "       [-0.13397652],\n",
      "       [ 0.16903265],\n",
      "       [ 0.00398427],\n",
      "       [-0.04428319],\n",
      "       [-0.86153054],\n",
      "       [-0.8288295 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Cast our training data as tensors\n",
    "Xt = tf.constant(X1trainS, dtype=tf.float32)\n",
    "Tt = tf.constant(Ttrain, dtype=tf.float32)\n",
    "\n",
    "# Notice we use tf.Variable which works just like tf.constant but \n",
    "# now we can change the value of the tensor using w.assign().\n",
    "w = tf.Variable(tf.random.uniform([D + 1, 1], -1.0, 1.0, seed=42), name=\"w\")\n",
    "\n",
    "print(\"Randomly initialized weights: {}\".format(w))\n",
    "for epoch in range(n_epochs):\n",
    "    Yt = tf.matmul(Xt, w, name=\"predictions\")\n",
    "    error = Yt - Tt\n",
    "    # Compute the loss function\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    # TODO (1): Compute the gradient of the loss function\n",
    "    gradients = \n",
    "    # TODO (2): Update weights given gradients\n",
    "    w.assign()\n",
    "    \n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch\", epoch, \"MSE = {}\".format(mse))\n",
    "\n",
    "print(w)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Autograd\n",
    "Instead of manually computing the gradients, TensorFlow can actually do it for us. To do so, we have to use`tf.GradientTape()` which records the operations used during a specified block of code. The recorded operations are then used to compute the automatic differentiation. Let's start by stepping through what is happening with the below code.\n",
    "\n",
    "`with tf.GradientTape() as tape:\n",
    "      <code block>\n",
    "`\n",
    "\n",
    "The above line uses a Python [with statement](https://www.python.org/dev/peps/pep-0343/) in conjunction with `tf.GradientTape()`. Essentially, this line aliases `tf.GradientTape()` to `tape` and activates `tf.GradientTape()` to track operations **only** within the proceeding code block (represented by `<code block>`). The `<code block>` is replaced with the MSE calculation code below. \n",
    "\n",
    "`\n",
    "Yt = tf.matmul(Xt, w, name=\"predictions\")\n",
    "error = Yt - Tt\n",
    "loss = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "`\n",
    "\n",
    "Here we calculate all the necessary variables to compute our MSE loss function (predictions and error). In doing so `tf.GradientTape()` can track all the appropriate variables and operations in order to compute the gradient of our loss function.\n",
    "\n",
    "Now, when we call `tape.gradient(loss, w)`,outside the `tf.GradientTape` code block, TensorFlow will automatically compute our gradients. Our gradients are calculated based on our loss function `loss` with respect to our trainable variable weights `w`. FYI, when you call `tape.gradient()`, the recorded tape will be automatically erased. If you call `tape.gradient()` twice, it will throw an exception!\n",
    "\n",
    "Finally, our weight update `w.assign(w - learning_rate * gradients)` is exactly the same as before.\n",
    "\n",
    "### References\n",
    "- [Intro to tf.GradientTape](https://medium.com/analytics-vidhya/tf-gradienttape-explained-for-keras-users-cc3f06276f22)\n",
    "- [TensorFlow automatic differentiation](https://www.tensorflow.org/tutorials/customization/autodiff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(9, 1) dtype=float32, numpy=\n",
      "array([[-0.12018752],\n",
      "       [-0.39934063],\n",
      "       [ 0.33478546],\n",
      "       [-0.25189137],\n",
      "       [-0.753314  ],\n",
      "       [ 0.3486998 ],\n",
      "       [ 0.23956943],\n",
      "       [ 0.8812921 ],\n",
      "       [ 0.7901039 ]], dtype=float32)>\n",
      "Epoch 0 MSE = 8.58199691772461\n",
      "Epoch 100 MSE = 1.0080363750457764\n",
      "Epoch 200 MSE = 0.8041122555732727\n",
      "Epoch 300 MSE = 0.728164553642273\n",
      "Epoch 400 MSE = 0.6742595434188843\n",
      "Epoch 500 MSE = 0.6350277662277222\n",
      "Epoch 600 MSE = 0.6063775420188904\n",
      "Epoch 700 MSE = 0.5853891372680664\n",
      "Epoch 800 MSE = 0.5699597597122192\n",
      "Epoch 900 MSE = 0.5585735440254211\n",
      "<tf.Variable 'w:0' shape=(9, 1) dtype=float32, numpy=\n",
      "array([[ 2.0708354 ],\n",
      "       [ 0.8786321 ],\n",
      "       [ 0.17053525],\n",
      "       [-0.26282772],\n",
      "       [ 0.25052705],\n",
      "       [ 0.01857397],\n",
      "       [-0.05218828],\n",
      "       [-0.43829185],\n",
      "       [-0.4146419 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Cast our training data as tensors\n",
    "Xt = tf.constant(X1trainS, dtype=tf.float32)\n",
    "Tt = tf.constant(Ttrain, dtype=tf.float32)\n",
    "\n",
    "# Notice we use tf.Variable which works just like tf.constant but \n",
    "# now we can change the value of the tensor using w.assign().\n",
    "w = tf.Variable(tf.random.uniform([D + 1, 1], -1.0, 1.0, seed=42), name=\"w\")\n",
    "\n",
    "print(w)\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        Yt = tf.matmul(Xt, w, name=\"predictions\")\n",
    "        error = Yt - Tt\n",
    "        loss = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "\n",
    "    gradients = tape.gradient(loss, w)\n",
    "    w.assign(w - learning_rate * gradients)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch\", epoch, \"MSE = {}\".format(loss))\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "# Making Predictions\n",
    "\n",
    "So far we have only seen what training is like with \"low-level\" TensorFlow. So, how do we compute our predictions? It's simple! Just like before we want to take the dot product between our learned weights `w` and our test data `X1testS`.\n",
    "\n",
    "### TODO:\n",
    "1. Compute the predictions for our test data `X1testS` using the tensor version of our test data `Xt` and our learned weights `w`.\n",
    "    1. Hint: Remember `tf.matmul()` is our dot product function for tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def mse_loss(y, t):\n",
    "    return tf.reduce_mean(tf.square(y-t), name=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.5514115691184998\n"
     ]
    }
   ],
   "source": [
    "Xt = tf.constant(X1testS, dtype=tf.float32)\n",
    "Tt = tf.constant(Ttest,  dtype=tf.float32)\n",
    "# TODO (1)\n",
    "y = \n",
    "print(\"MSE: {}\".format(mse_loss(y, Tt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Build a TensorFlow Keras Model\n",
    "\n",
    "Now it's time to get into the high-level TensorFlow API. As of TensorFlow 2.0, TensorFlow has fully integrated a high-level API called Keras. Keras makes creating, training, and testing models easy and quick. Here we'll rewrite our linear model with Keras and start logging with TensorBoard Keras callback function. The logged information can later be visualized with TensorBoard which is a great tool for debugging and analysis.\n",
    "\n",
    "### References:\n",
    "- [TensorFlow Keras Tutorial](https://www.tensorflow.org/guide/keras)\n",
    "- [Training and evaluating models with Keras](https://www.tensorflow.org/guide/keras/train_and_evaluate)\n",
    "- [Keras Model Docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model)\n",
    "- [Keras Layers Docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers)\n",
    "- [Keras Metrics Docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics)\n",
    "- [TensorBoard Tutorial](https://www.tensorflow.org/tensorboard/get_started)\n",
    "- [Save and Load Models Docs](https://www.tensorflow.org/tutorials/keras/save_and_load)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define our log file path that will be used by Keras to track our TensorFlow computational graph, loss, and any other variables we might specify. When we want to visualize the information in the this log file we will use TensorBoard!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tf_logs/run-20200317171351/'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "logdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to build our model. Here is what is happening in the model building code below.\n",
    "\n",
    "`\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "`\n",
    "\n",
    "The above line declares a very basic model using Keras `Sequential` class, which doesn't require you to implement your own model class (unlike implementations shown on the Keras [model docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model)).  We create a fully connected layer, i.e. a hidden layer, using `tf.keras.layers.Dense()`. By creating only one layer and giving this layer only one hidden unit we are essentially creating a linear regression model! There are other layer types that Keras provides and you can even implement your own. The Keras [layer docs](https://www.tensorflow.org/api_docs/python/tf/keras/layers) provide a comprehensive list of all the implemented layers.\n",
    "\n",
    "`\n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=0.001))\n",
    "`\n",
    "\n",
    "When we are ready to build our model we call the `.compile()` method. When calling this method we have to pass the type of loss we want to use and the type of optimizer (these can be strings or objects). In our case we are using MSE as our loss and stochastic gradient decent as our optimizer with a .001 learning rate. Additionally, we can pass other metrics that will automatically be tracked for us. The Keras [metric docs](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) provide a list of the metrics you can pass. There are other parameters to pass as well so check out the [model docs](https://www.tensorflow.org/api_docs/python/tf/keras/Model) to see what else you can do. \n",
    "\n",
    "`tensorboard_cb = tf.keras.callbacks.TensorBoard(logdir)`\n",
    "\n",
    "Lastly, we initialize a Keras TensorBoard callback so we can visualize our graph and variables later on with TensorBoard. This callback essentially alerts Keras to take care of all the TensorBoard tracking for us and will be passed as a callback parameter when we use `.fit()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Build network structure\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Build model with loss and optimizer \n",
    "model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=0.001))\n",
    "\n",
    "# Create a tensorboard callback for keras so we can visualize our graph and variables later with tensorboard later\n",
    "tensorboard_cb = tf.keras.callbacks.TensorBoard(logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model is compiled it's time to train our model using the `.fit()` method. Here we need to pass our data and targets. We also have the option to pass other parameters, like the number of training epochs or Keras callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16512 samples\n",
      "Epoch 1/30\n",
      "16512/16512 [==============================] - 0s 26us/sample - loss: 3.6557\n",
      "Epoch 2/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.8955\n",
      "Epoch 3/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.6937\n",
      "Epoch 4/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.6439\n",
      "Epoch 5/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.6200\n",
      "Epoch 6/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.6034\n",
      "Epoch 7/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5907\n",
      "Epoch 8/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5799\n",
      "Epoch 9/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5711\n",
      "Epoch 10/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5637\n",
      "Epoch 11/30\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5574\n",
      "Epoch 12/30\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5522\n",
      "Epoch 13/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5478\n",
      "Epoch 14/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5441\n",
      "Epoch 15/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5410\n",
      "Epoch 16/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5386\n",
      "Epoch 17/30\n",
      "16512/16512 [==============================] - 0s 18us/sample - loss: 0.5363\n",
      "Epoch 18/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5346\n",
      "Epoch 19/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5330\n",
      "Epoch 20/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5318\n",
      "Epoch 21/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5307\n",
      "Epoch 22/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5297\n",
      "Epoch 23/30\n",
      "16512/16512 [==============================] - 0s 20us/sample - loss: 0.5289\n",
      "Epoch 24/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5285\n",
      "Epoch 25/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5279\n",
      "Epoch 26/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5276\n",
      "Epoch 27/30\n",
      "16512/16512 [==============================] - 0s 21us/sample - loss: 0.5270\n",
      "Epoch 28/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5267\n",
      "Epoch 29/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5263\n",
      "Epoch 30/30\n",
      "16512/16512 [==============================] - 0s 19us/sample - loss: 0.5264\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X1trainS, Ttrain, epochs=30, callbacks=[tensorboard_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.fit()` method returns a `Histroy` object which tracks parameters, loss, metrics and other specified stats. For instance, we can see all stats that were tracked by using `history.history.keys()`. In our case, our only key is 'loss'. Thus, we can only view the history of our loss for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss'])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.6556750716165056,\n",
       " 0.8955351624262425,\n",
       " 0.6936791785234628,\n",
       " 0.6438602163752382,\n",
       " 0.6200437917150268,\n",
       " 0.6033585683781971,\n",
       " 0.590692140277504,\n",
       " 0.5799468215121779,\n",
       " 0.5710935985683933,\n",
       " 0.5636743671845558,\n",
       " 0.557428067651137,\n",
       " 0.5522487776050734,\n",
       " 0.5478149893325429,\n",
       " 0.5441364530733851,\n",
       " 0.5409962070831494,\n",
       " 0.5386274226240871,\n",
       " 0.5362597447495128,\n",
       " 0.5346140348922837,\n",
       " 0.5330358995774458,\n",
       " 0.5317792033906593,\n",
       " 0.5307304124788258,\n",
       " 0.5297229994348315,\n",
       " 0.5289229892136514,\n",
       " 0.528545220635196,\n",
       " 0.5278676256768463,\n",
       " 0.5276472680270672,\n",
       " 0.5269970900501854,\n",
       " 0.5267061214287614,\n",
       " 0.5262970242562682,\n",
       " 0.5264100913218287]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when we need to test our results we have two options. We can call `.predict()` that will return our predictions. We can also call `.evaluate()` that will return our loss and any metrics we passed to our model when we called `.compile()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.8542817 ],\n",
       "       [2.2435832 ],\n",
       "       [0.85494673],\n",
       "       ...,\n",
       "       [1.5509338 ],\n",
       "       [1.9913034 ],\n",
       "       [0.8693051 ]], dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = model.predict(X1testS)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4128/4128 [==============================] - 0s 14us/sample - loss: 0.5267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5267296135425568"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error = model.evaluate(X1testS, Ttest)\n",
    "error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can run TensorBoard using the `%load_ext tensorboard` and `%tensorboard` Jupyter Magics. Notice, we have to specify the log directory 'tf_logs/' that we created earlier. We also can specify the port. By default the port is set to 6006 so specifying it again is somewhat redundant here. \n",
    "\n",
    "When TensorBoard first starts it will default you to the 'SCALARS' main menu tab, which plots any of your tracked stats. You can view the computational graph by clicking on the 'GRAPHS' tab.\n",
    "\n",
    "TensorBoard will by default load all the log files in the 'tf_logs/' directory. You can view all the loaded log files under 'Runs' where you can freely uncheck log files you don't want to display. If you want to read more on TensorBoard check out the [offical tutorial](https://www.tensorflow.org/tensorboard/get_started)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 9711), started 2:58:13 ago. (Use '!kill 9711' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-b36f678dea90a3f7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-b36f678dea90a3f7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --port 6006 --logdir tf_logs/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
