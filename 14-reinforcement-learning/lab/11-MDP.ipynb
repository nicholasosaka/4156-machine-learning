{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\yv}{\\mathbf{y}}\n",
    " \\newcommand{\\zv}{\\mathbf{z}}\n",
    " \\newcommand{\\Chi}{\\mathcal{X}}\n",
    " \\newcommand{\\R}{\\rm I\\!R}\n",
    " \\newcommand{\\sign}{\\text{sign}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    " \\newcommand{\\Zm}{\\mathbf{Z}}\n",
    " \\newcommand{\\I}{\\mathbf{I}}\n",
    " \\newcommand{\\muv}{\\boldsymbol\\mu}\n",
    " \\newcommand{\\Sigmav}{\\boldsymbol\\Sigma}\n",
    "$\n",
    "\n",
    "\n",
    "\n",
    "# Markov Decision Processes\n",
    "\n",
    "<br/>\n",
    "<br/><br/><br/>\n",
    "\n",
    "### ITCS 4156\n",
    "\n",
    "### Minwoo \"Jake\" Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this activity is to define an MDP for a the discrete marble task and then use dynamic programming to solve it. Follow the TODO titles and comments to finish the activity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Agenda\n",
    "\n",
    "1. Discrete Marble Task\n",
    "    1. Environment and Actions\n",
    "2. DMarble Environment Class\n",
    "3. Policy Iteration\n",
    "    1. Notation\n",
    "    2. Algorithm Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "from copy import deepcopy as copy\n",
    "\n",
    "# To plot pretty figures\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "import IPython.display as ipd  # for display and clear_output\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "# print only 3 digits under decimal point\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Discrete Marble Task \n",
    "\n",
    "The discrete marble task is actually a rather simple task. The goal of this task is to move a marble from any given location on a 1-D line to the goal location. The marble is restricted to discrete actions of move left, right, or don't move (no action).\n",
    "\n",
    "## Environment and Actions\n",
    "For this task our environment is defined by a marble on a 1-D line. This \"1-D line\" also defines our possibles actions as the following: move left, move right, or don't move (no action). Each action is defined as a discrete number: -1 (left), 0 (don't move), or 1 (right). These discrete numbers also determine the distance the marble moves on the 1-D line. This means each state must be defined by an integer! In our case, our state representations will range between 0 and 10 (see below image). At some point on the 1-D line there is also a goal state. Thus, the goal of the task is to explore the environment to find the goal state.\n",
    "\n",
    "If you're still confused on the environment setup see the picture below. Here you can see the marble in red with its 3 possible actions. Each move left or right actions moves the marble 1 unit (left or right) on the 1-D line. The goal state location is at 5 and the starting marble state location is at 3. Thus, the marble has to perform 2 move right actions to reach the goal state. \n",
    "\n",
    "<img src=\"https://i.imgur.com/MwXDZPI.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To start off, we must define the MDP before we can solve the discrete marble task. Let's define what our representations will be for the standard MDP model $(S, A, P, R, \\gamma)$.\n",
    "\n",
    "\n",
    "# TODO:\n",
    "\n",
    "Answer the below questions or fill in the blanks (fill in the blanks are indicated by the **[blank]** symbol). Add your answer in the markdown cell underneath the question. Note, these should be plain text answers, no coding is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (1)**\n",
    "\n",
    "What are all the possible states $S$ the marble could be in?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (1) ANSWER**\n",
    "\n",
    "$S$ consists of states $0$ through $10$, inclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (2)**\n",
    "\n",
    "The three actions $A$ are **[blank]** and they are represented by the three integers **[blank]**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (2) ANSWER**\n",
    "\n",
    "Movement to the left and right (and the lack thereof) and they are represented by $\\{-1,0,1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (3)**\n",
    "\n",
    "The transition probabilities $T$ in the environment are deterministic, so all transition probabilities are **[blank]**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (3) ANSWER**\n",
    "\n",
    "static, and known in advance. (I think this is what the question was aiming for?)\n",
    "\n",
    "In our case, the probabilities for a transition of states are equally distributed across all three actions, meaning each action has a $0.33$ chance of occuring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (4)**\n",
    "\n",
    "The reward $R$ is given at the end of the task when the marble reaches the goal. Thus, we can say $R$ equals to **[blank]** when the marble reaches goal, otherwise $R$ is equal to **[blank]**. Hint, our reward function $R$ is binary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO (4) ANSWER**\n",
    "\n",
    "$R = 1$ when at the goal state, otherwise $R = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DMarble Environment Class\n",
    "\n",
    "Now it's time to implement the discrete marble task! Below we have defined a class called `DMarble` that defines the discrete marble task. Be sure to read through this class and understand what each method is doing, as we'll be using them later. There are a few TODOs you need complete within `DMarble` class as well.\n",
    "\n",
    "### General Python Note: Private Methods and Variables\n",
    "Note, that any method/variable that has a leading underscore `_` indicates that said method/variable is private. Python doesn't have true private methods/variables like other languages, rather it has pseudo private methods/variables. This means you can still access any private method/variable marked with a leading underscore. However, the leading underscore indicates to users that the method/variable should be treated as if it was a true private method/variable (meaning, the user should not call the method/variable from outside the class or things might go wrong later on).\n",
    "\n",
    "For more information see this GeeksforGeeks [post](https://www.geeksforgeeks.org/private-variables-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "#### init() method\n",
    "1. If `start` is None, not given, then set our start position `cur_state` to be random state between our state representation bounds, given by the class variable `self.bounds`. Remember, our lowest possible state representation is 0 and the highest possible state representation is 10.\n",
    "    1. Hint: Use [`np.random.randint()` function](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randint.html) to generate a random integer between two bounds. Be sure to add 1 to the end bound since the upper bound of `np.random.randint()` is  exclusive!\n",
    "2. Else, if `start` is not `None`, is given, then set `cur_state` equal to `state`.\n",
    "\n",
    "#### next() method\n",
    "3. Generate our next state `s1` by adding the input action `a` to the input state `s`. Remember, our state representations are integers!\n",
    "\n",
    "#### _get_reward() method\n",
    "4. Generate the reward function by creating an if statement to check if the next state input  is the goal state.That is, if `s1` is equal `self.goal` then return a reward of 1, else return a reward of 0.\n",
    "    1. Hint: Python let's you write one line if-else statements for short if statements like the above. Check out this [stack overflow post](https://stackoverflow.com/questions/2802726/putting-a-simple-if-then-else-statement-on-one-line) for how one line if-else statements work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DMarble(object):\n",
    "    \"\"\" Reinforcement learning Model for Discrete Marble. \n",
    "    \n",
    "        states: x, dx\n",
    "        action: action [-1, 0, 1]\n",
    "\n",
    "        |            ___                     |\n",
    "        |___________|///|____G_______________|\n",
    "                    <- ->    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, goal=5, **params):\n",
    "        \"\"\"  create an object with the given goal\n",
    "        \n",
    "            attributes\n",
    "            ===========\n",
    "            act_bound   list\n",
    "                        the bounds for actions \n",
    "            bound       list\n",
    "                        the bounds for states \n",
    "            goal        list\n",
    "                        the goal location\n",
    "        \"\"\"\n",
    "        self.act_bound = [-1, 1]\n",
    "        self.bound = [0, 10]\n",
    "        self.goal = goal\n",
    "\n",
    "    def init(self, start=None):\n",
    "        \"\"\" initialize the starting position\n",
    "        \n",
    "            paramters\n",
    "            ----------\n",
    "            start   int\n",
    "                    the start position (optional). if not given, random\n",
    "            @return current state\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        if start is None:\n",
    "            # TODO (1)\n",
    "            cur_state = np.random.randint(0,11)\n",
    "        else:\n",
    "            # TODO (2)\n",
    "            cur_state = start\n",
    "        return cur_state\n",
    "       \n",
    "   \n",
    "    def next(self, s, a):\n",
    "        \"\"\" simulate 1 step by taking the action from the given state \n",
    "            it updates the current states (_cur_state) and \n",
    "            flag variable if it ends the episode (_done)\n",
    "        \n",
    "            paramters\n",
    "            ----------\n",
    "            s       int\n",
    "                    current state\n",
    "            a       int\n",
    "                    action \n",
    "            @return list of next state, reward, done\n",
    "        \"\"\"\n",
    "        # TODO (3)\n",
    "        s1 = s + a\n",
    "        \n",
    "        # check if it hits the ends \n",
    "        if s1 < self.bound[0]:\n",
    "            s1 = self.bound[0]\n",
    "        elif s1 > self.bound[1]:\n",
    "            s1 = self.bound[1] \n",
    "        r1 = self._get_reward(s1)\n",
    "        \n",
    "        return s1, r1\n",
    "    \n",
    "    ###### getters #############################\n",
    "    \n",
    "    ### reward function \n",
    "    def _get_reward(self, s1):\n",
    "        # TODO (4)\n",
    "        return 1 if s1 == self.goal else 0\n",
    "    \n",
    "    ### function to check the end condition\n",
    "    def is_done(self, s):\n",
    "        return s == self.goal\n",
    "\n",
    "    ### get all the possible actions\n",
    "    def get_actions(self):\n",
    "        # return all the possible actions in a list\n",
    "        return [-1, 0, 1]\n",
    "    \n",
    "    ### get a random action\n",
    "    def get_random_action(self):\n",
    "        # return randomly selected action \n",
    "        return np.random.choice(self.get_actions())\n",
    "    \n",
    "    ### get the number of states\n",
    "    def get_n_states(self):\n",
    "        return self.bound[1] - self.bound[0] + 1\n",
    "    \n",
    "    ### get the number of actions\n",
    "    def get_n_actions(self):\n",
    "        return len(self.get_actions())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create an object from the DMarble class and simulate/test if it works. By default, our goal location is set to 5 if we do not pass a value for the `goal` parameter. Regardless, below we are explicitly setting the goal location to 5 for ease of reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = DMarble(goal=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    " Before we solve this task let's manually interact with the environment. To do so, let's manually input the start location and actions until we reach the goal state.\n",
    " \n",
    "1. Set the initial start location of the marble to 3 using the `.init()` method. Store the output into `s`. \n",
    "2. Now apply sequence of actions -1, 1, 0, 1, 1 and observe states the marble enters. Store every output into `s` and `r`.\n",
    "    1. Take a move left action (-1) \n",
    "    2. Take a move right action (1) \n",
    "    3. Take a no action (0)\n",
    "    4. Take a move right action (1) action\n",
    "    5. Take a move right action (1) action\n",
    "  \n",
    "3. Check if we have reached the goal state using the `.is_done()` method. Store the output into `reached_goal`.\n",
    "    1. Hint: You must pass the current state `s` to this method!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (1)\n",
    "s = env.init(3)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2A)\n",
    "s, r = env.next(s, -1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2B)\n",
    "s, r = env.next(s, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2C)\n",
    "s, r = env.next(s, 0)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2D)\n",
    "s, r = env.next(s, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2E)\n",
    "s, r = env.next(s, 1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Have we reached the goal? True\n"
     ]
    }
   ],
   "source": [
    "# TODO (3)\n",
    "reached_goal = env.is_done(s)\n",
    "print(\"Have we reached the goal? {}\".format(reached_goal))\n",
    "assert reached_goal == True "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Now, let's implement the policy iteration algorithm to solve the discrete marble task. To remind you, here is the copy of the class notes and some pseudo code to summarize the algorithm. \n",
    "\n",
    "\n",
    "![](http://incompleteideas.net/sutton/book/first/4/img158.gif)\n",
    "\n",
    "## Notation\n",
    "- S: a finite set of states\n",
    "- A: a finite set of actions\n",
    "- P: a state transition probability\n",
    "    - $P^a_{ss′}= P[S_{t+1}=s′|S_t=s,A_t=a]$\n",
    "- R: a reward function\n",
    "- γ: a discount factor (gamma)\n",
    "\n",
    "## Algorithm Summary\n",
    "* Start with an initial policy $\\pi^0$.\n",
    "* Iteratively,\n",
    "    * Evaluate policy ($V^n(x) = V^{\\pi^n}(x)$):\n",
    "    $$ V^n(s) = R(s, a = \\pi^n(s)) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a = \\pi^n(s)) V^{n}(s^\\prime) $$\n",
    "    * Improve policy:\n",
    "    $$ \\pi_{n+1}(s) = \\arg \\max_a \\Big[ R(s, a) + \\gamma \\sum_{s^\\prime} P(s^\\prime | s, a) V^{n}(s^\\prime) \\Big] $$\n",
    "    \n",
    "* Stop condition:\n",
    "    * Policy does not change anymore (for about 10 iterations)\n",
    "    * The changes in evaluation of values are minor \n",
    " \n",
    "The psuedo code is located in the below image. Note, $P(s^\\prime | s, a) $ above and $P(s^\\prime, r | s, a)$ in the below image can be treated as the same thing. You'll also notice the equations are written slightly different, either form is okay.\n",
    "![](https://miro.medium.com/max/2952/1*ZYq8bX-q4Z8-8aPndQaz9w.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "\n",
    "Implement the policy iteration algorithm here. Here, our goal state is located at state 5, since we are reusing the same `env` object as before, and our starting state will be randomized (really we'll be treating every state as a starting state). Read the code and fill in the TODOs.\n",
    "\n",
    "#### Policy Evaluation\n",
    "1. Execute the current action using the `.next()` method by pass current state `s` and action. The current action is determined by indexing `As` (the set of all actions) at `a` (the current action index of `As`). Store the new state and reward into `s1` and `r1`.\n",
    "\n",
    "2. Compute the value of the given state by computing the Policy Evaluation equation $r + \\gamma V(s')$. To do so multiple, `gamma` by the next state value (done by indexing `V` at `s1`), then add our reward for entering the next state `r1`. Remember, our state transition probability is 1, since the state transition $P(s^\\prime | s, a)$ is deterministic (we can only go to one other state from any given state)! Store output into `val`.\n",
    "\n",
    "\n",
    "#### Policy Improvement\n",
    "3. Execute the current action using the `.next()` method by pass current state `s` and action. The current action is determined by indexing `As` (the set of all actions) at `a` (the current action index of `As`). Store the new state and reward into `s1` and `r1`.\n",
    "\n",
    "4. Compute the value of the given state by computing the Policy Improvement equation $r + \\gamma V(s')$. To do so multiple, `gamma` by the next state value (done by indexing `V` at `s1`), then add our reward for entering the next state `r1`. Store the output into `val`\n",
    "\n",
    "5. Pick the action with the highest probability by taking the `np.argmax()` of `values`. Store the output into `new_a`.\n",
    "    1. Note: `values` is a 1-d list so no axis parameter is needed for `np.argmax()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialzed Values\n",
      "Starting state values:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state 0</th>\n",
       "      <th>state 1</th>\n",
       "      <th>state 2</th>\n",
       "      <th>state 3</th>\n",
       "      <th>state 4</th>\n",
       "      <th>state 5</th>\n",
       "      <th>state 6</th>\n",
       "      <th>state 7</th>\n",
       "      <th>state 8</th>\n",
       "      <th>state 9</th>\n",
       "      <th>state 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>values</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting stochastic policy probabilities:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left</th>\n",
       "      <th>Don't Move</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>state 0</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 1</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 3</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 4</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 5</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 6</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 7</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 8</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 9</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 10</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration #1\n",
      "[0.    0.    0.    0.    0.333 0.463 0.614 0.239 0.093 0.036 0.018]\n",
      "[0.    0.    0.    0.1   0.511 0.717 0.785 0.334 0.141 0.061 0.031]\n",
      "[0.    0.    0.03  0.165 0.613 0.808 0.849 0.373 0.164 0.073 0.037]\n",
      "[0.    0.009 0.053 0.205 0.655 0.844 0.874 0.39  0.174 0.079 0.04 ]\n",
      "[0.003 0.017 0.068 0.223 0.673 0.858 0.885 0.397 0.179 0.082 0.041]\n",
      "[0.005 0.022 0.076 0.232 0.681 0.864 0.89  0.401 0.181 0.083 0.042]\n",
      "[0.007 0.025 0.079 0.235 0.684 0.867 0.892 0.402 0.182 0.083 0.042]\n",
      "[0.008 0.027 0.081 0.237 0.686 0.868 0.893 0.403 0.182 0.084 0.042]\n",
      "[0.008 0.027 0.082 0.238 0.687 0.869 0.893 0.403 0.182 0.084 0.042]\n",
      "\n",
      "\n",
      "Policy before Policy Improvement:\n",
      " [[0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]\n",
      " [0.333 0.333 0.333]]\n",
      "Policy after Policy Improvement:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Iteration #2\n",
      "[0.025 0.074 0.214 0.618 1.782 1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[0.066 0.193 0.556 1.604 1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[0.173 0.5   1.443 1.71  1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[0.45  1.299 1.539 1.71  1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[1.169 1.385 1.539 1.71  1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[1.247 1.385 1.539 1.71  1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "[1.247 1.385 1.539 1.71  1.9   1.    1.9   1.71  1.539 1.385 1.247]\n",
      "\n",
      "\n",
      "Policy before Policy Improvement:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "Policy after Policy Improvement:\n",
      " [[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Policy is stable now.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Defin discounting factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Define threshold for convergence\n",
    "thresh = 0.001\n",
    "\n",
    "# Reinitialize the environment\n",
    "env.init()\n",
    "As = env.get_actions() # all the actions\n",
    "\n",
    "# Get number of states and actions\n",
    "n_s = env.get_n_states()\n",
    "n_a = env.get_n_actions()\n",
    "\n",
    "# Define state value table\n",
    "V = np.zeros(env.get_n_states())\n",
    "\n",
    "# Initialize a stochastic policy so that our algorithm will explore\n",
    "# all actions for each state during the first iteration. Doing so\n",
    "# means we have to add an action probability to our Policy Evaluation equation!\n",
    "pi = np.ones((n_s, n_a)) / n_a\n",
    "\n",
    "# Build fancy Pandas DataFrames for visualizing initial state value \n",
    "# array (V) and policy array (pi).\n",
    "V_columns = [ \"state {}\".format(s) for s in np.arange(0, 11)]\n",
    "V_df = pd.DataFrame(V.reshape(1, -1), columns=V_columns)\n",
    "V_df.index = ['values']\n",
    "\n",
    "pi_columns = ['Left', \"Don't Move\", 'Right']\n",
    "pi_rows = [\"state {}\".format(s) for s in np.arange(0, 11)]\n",
    "pi_df = pd.DataFrame(pi, columns=pi_columns)\n",
    "pi_df.index = pi_rows\n",
    "\n",
    "print (\"Initialzed Values\")\n",
    "print(\"Starting state values:\")\n",
    "display(HTML(V_df .to_html()))\n",
    "print(\"Starting stochastic policy probabilities:\")\n",
    "display(HTML(pi_df.to_html()))\n",
    "print(\"\\n\\n\\n\")\n",
    "\n",
    "# Policy Iteration \n",
    "i = 0\n",
    "while True:\n",
    "    print (\"Iteration #{}\".format(i+1))\n",
    "    i += 1\n",
    "    \n",
    "    ############################################\n",
    "    # Policy Evaluation (ONLY updating our state values V)\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        delta = 0\n",
    "        \n",
    "        # Evaluate for all states\n",
    "        for s in range(n_s):\n",
    "            v = V[s] # store previous state values\n",
    "            V[s] = 0 # reset state value so new value can be calculated\n",
    "            \n",
    "            # Evaluate for all actions given a state for the initial pi (stochastic).\n",
    "            # Once we apply our first policy improvement then we are only updating \n",
    "            # a signle deterministic action.\n",
    "            for a in range(n_a):\n",
    "                # TODO (1):\n",
    "                s1, r1 = env.next(s, As[a])\n",
    "                # TODO (2): action probability * (reward + discounting factor * V')\n",
    "                val = r1 + gamma * V[s1]\n",
    "                V[s] += pi[s, a] * val\n",
    "                \n",
    "            # Compute difference between previous and current state values\n",
    "            delta = max(delta, abs(V[s] - v))\n",
    "            \n",
    "        # Check if the difference in our previous and current state values is\n",
    "        # smaller than the defined threshold for convergence. \n",
    "        # If so, then our state values have converged for this iteration!\n",
    "        if delta < thresh:\n",
    "            converged = True\n",
    "            \n",
    "        print(V)        \n",
    "    print('\\n')\n",
    "    print(\"Policy before Policy Improvement:\\n {}\".format(pi))\n",
    "    ############################################\n",
    "    # Policy Improvement (ONLY updating our policy pi USING V)\n",
    "    stable = True\n",
    "    for s in range(n_s):\n",
    "        old_a = np.argmax(pi[s])  # store current policy\n",
    "        \n",
    "        values = []\n",
    "        # Compute the values for each action for a given state\n",
    "        for a in range(n_a):\n",
    "            # TODO (3)\n",
    "            s1, r1 = env.next(s, As[a])\n",
    "            # TODO (4)\n",
    "            val = r1 + gamma * V[s1]\n",
    "            values.append(val)\n",
    "        \n",
    "        # TODO (5)\n",
    "        new_a = np.argmax(values)\n",
    "        \n",
    "        # If the old action does not equal the new action \n",
    "        # then we have not stablized yet.\n",
    "        if old_a != new_a:\n",
    "            stable = False\n",
    "        \n",
    "        # Update the policy to a deterministic policy.\n",
    "        # Meaning, one action has a value of 1 while\n",
    "        # the rest have 0 as their value.\n",
    "        pi[s] = np.eye(n_a)[new_a]\n",
    "    \n",
    "    print(\"Policy after Policy Improvement:\\n {}\".format(pi))\n",
    "    print(\"\\n\\n\\n\")\n",
    "    if stable:\n",
    "        print(\"Policy is stable now.\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the task is rather simple, the algorithm can quickly find the solution. The printouts show how the state values are updated until convergence. The state value convergence looks reasonable, as you can see the state values increase as the states get closer to the goal (the goal is at state 5, so the state values increase as they approach it).\n",
    "\n",
    "We can also see how our stochastically initialized policy changes to a deterministic policy by looking at the before and after policy improvement printouts. In addition, we can see our policy quickly converges. However, the algorithm doesn't stop until the second iteration, where we can finally assert that our before and after policy improvement policies don't change.\n",
    "\n",
    "If we print the our final policy $\\pi$, we can see the optimal actions to take to reach the goal node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Left</th>\n",
       "      <th>Don't Move</th>\n",
       "      <th>Right</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>state 0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state 10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Left  Don't Move  Right\n",
       "state 0      0           0      1\n",
       "state 1      0           0      1\n",
       "state 2      0           0      1\n",
       "state 3      0           0      1\n",
       "state 4      0           0      1\n",
       "state 5      0           1      0\n",
       "state 6      1           0      0\n",
       "state 7      1           0      0\n",
       "state 8      1           0      0\n",
       "state 9      1           0      0\n",
       "state 10     1           0      0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi_columns = ['Left', \"Don't Move\", 'Right']\n",
    "pi_rows = [\"state {}\".format(s) for s in np.arange(0, 11)]\n",
    "pi_df = pd.DataFrame(pi, columns=pi_columns, dtype=int)\n",
    "pi_df.index = pi_rows\n",
    "pi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By taking the argmax of `pi` and indexing our environment's actions `env.get_actions()` we can get the actions the optimal policy would take in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state 0</th>\n",
       "      <th>state 1</th>\n",
       "      <th>state 2</th>\n",
       "      <th>state 3</th>\n",
       "      <th>state 4</th>\n",
       "      <th>state 5</th>\n",
       "      <th>state 6</th>\n",
       "      <th>state 7</th>\n",
       "      <th>state 8</th>\n",
       "      <th>state 9</th>\n",
       "      <th>state 10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aciton values</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               state 0  state 1  state 2  state 3  state 4  state 5  state 6  \\\n",
       "aciton values        1        1        1        1        1        0       -1   \n",
       "\n",
       "               state 7  state 8  state 9  state 10  \n",
       "aciton values       -1       -1       -1        -1  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_policy_idx = np.argmax(pi, axis=1)\n",
    "opt_policy = np.array(env.get_actions())[opt_policy_idx]\n",
    "\n",
    "columns = [ \"state {}\".format(s) for s in np.arange(0, 11)]\n",
    "df = pd.DataFrame(opt_policy.reshape(1, -1), columns=columns)\n",
    "df.index = ['aciton values']\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
