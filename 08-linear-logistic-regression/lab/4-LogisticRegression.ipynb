{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    " \\newcommand{\\wv}{\\mathbf{w}}\n",
    " \\newcommand{\\Tm}{\\mathbf{T}}\n",
    " \\newcommand{\\Xm}{\\mathbf{X}}\n",
    "$\n",
    "\n",
    "# Logistic Regression (Workbook)\n",
    "\n",
    "### ITCS 4156\n",
    "### Minwoo \"Jake\" Lee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this activity is to implement the logistic regression and apply it to [iris dataset](https://archive.ics.uci.edu/ml/datasets/iris) that we used last week. In addition we will look at how to analyze our predictions with a confusion matrix, a popular method for quantifying and visualizing classification predictions. Follow the TODO titles and comments to finish the activity!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agenda\n",
    "\n",
    "* Iris Data: Loading and Partitioning\n",
    "    * Data Partitioning\n",
    "* Applying Logistic Regression\n",
    "    * Converting to Indicator Variables\n",
    "    * Logistic Regression Training\n",
    "* Applying New Metrics\n",
    "    * Confusion Matrix\n",
    "    * Precision, Recall, and F1-score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy as copy\n",
    "from IPython.core.debugger import set_trace\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Data: Loading and Partitioning\n",
    "Just like last week we will load the iris data via Scikit-learn. However, this week we will skip the detailed break down of the data loading and visualization. If you want to review the iris data loading process or the iris visualizations see last weeks lab on linear classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we did last week we import the iris data via sklearn.To do so we call the `load_iris()` function and store the `sklearn.utils.Bunch` output instance inside the varaible `iris`. We'll also go ahead and store the iris data into variables so we don't have to keep typing `iris.data` and `iris.target` every time we need to access the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "T = iris.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Partitioning\n",
    "\n",
    "The below code is the `partition()` function we used last week. Recall, that the `partition()` function takes as input the following Numpy arrays `X` and `T`. The output of  `partition()` is a list of data and targets where `Xs` holds our training and testing data partitions and `Ts` holds our training and testing target partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now partition the data \n",
    "\n",
    "\"\"\" partitioning data\n",
    "\n",
    "    parameters\n",
    "    -----------\n",
    "    X        numpy array\n",
    "             input data to partition\n",
    "    T        numpy array\n",
    "             target labels to partition\n",
    "    raito    list\n",
    "             list of ratios for partitions (should be summed to 1) \n",
    "             the number of return pairs are different\n",
    "    return\n",
    "    -------\n",
    "    \n",
    "    Xs       list of numpy arrays\n",
    "    \n",
    "    Ts       list of numpy arrays\n",
    "\"\"\"\n",
    "def partition(X, T, ratio=[0.8, 0.2]): \n",
    "    \n",
    "    # Checks to make sure ratio sums to 1\n",
    "    assert(np.sum(ratio) == 1)\n",
    "    \n",
    "    # Store the number of data samples \n",
    "    N = X.shape[0]\n",
    "\n",
    "    # change the 1d array to 2d if need\n",
    "    if len(T.shape) == 1:\n",
    "        T = T.reshape((N,1))\n",
    "    \n",
    "    # Shuffle the data indices \n",
    "    idxs = np.random.permutation(N)\n",
    "        \n",
    "    Xs = []\n",
    "    Ts = []\n",
    "    i = 0  # first index to zero\n",
    "    for k, r in enumerate(ratio):\n",
    "         # Number of rows that corresponds to kth element in ratios\n",
    "        nrows = int(round(N * r)) \n",
    "        \n",
    "        # print (i, nrows)\n",
    "        # If we are on the last ratio simply use the remaining data samples\n",
    "        if k == len(ratio)-1:\n",
    "            Xs.append(X[idxs[i:], :])\n",
    "            Ts.append(T[idxs[i:], :])\n",
    "        else:\n",
    "            Xs.append(X[idxs[i:i+nrows], :])\n",
    "            Ts.append(T[idxs[i:i+nrows], :])\n",
    "        \n",
    "        i += nrows\n",
    "    \n",
    "    return Xs, Ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_data():\n",
    "    \"\"\"Partition data into train and test splits.\"\"\"\n",
    "    global Xtrain, Xtest, Ttrain, Ttest\n",
    "    data, targets = partition(copy(X), copy(T))\n",
    "    # TODO: Fill in the right had side of the assignments below\n",
    "    Xtrain, Xtest = \n",
    "    # TODO: Fill in the right had side of the assignments below\n",
    "    Ttrain, Ttest = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape: (120, 4)\n",
      "Train target shape: (120, 1)\n",
      "Test data shape: (30, 4)\n",
      "Test target shape: (30, 1)\n"
     ]
    }
   ],
   "source": [
    "partition_data()\n",
    "print(\"Train data shape: {}\".format(Xtrain.shape))\n",
    "print(\"Train target shape: {}\".format(Ttrain.shape))\n",
    "print(\"Test data shape: {}\".format(Xtest.shape))\n",
    "print(\"Test target shape: {}\".format(Ttest.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Applying Logistic Regression \n",
    "\n",
    "Here we are, already on logistic regression - I  think this is the quickest we have ever made it to our algorithm! For this section make sure to reference the readings and lecture notes if you are getting stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Indicator Variables\n",
    "Well, maybe we aren't there yet. Just like last week we need to do some preprocessing of targets so they are compatible with logistic regression. \n",
    "\n",
    "Remember how we adjusted our targets to `[-1, 1]` specifically for the perceptron algorithm last week? Well, we’ll be doing another adjustment of our targets, but this time we need to convert our targets to indicator variables, also called one-hot encodings. \n",
    "\n",
    "Recall from the lecture notes and our previous lab on linear models that converting to indicator variables means converting all our targets to a row vector of 1s and 0s. To frame this in more concrete terms we want to assign each unique target to have its own column. This means that if we have three targets (0, 1, and 2) then we have three columns, one for each target. For example, if a given data sample's indicator variable has a 1 in the first column then the rest of the columns must contain 0s. The 1 in the first column indicates to us that our target is 0!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's recall how many unique targets our iris data has by calling the Numpy `unique()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(Ttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, just like last week our number of targets is 3. Remember, target 0 corresponds to setosa, target 1 corresponds to versicolor, and target 2 corresponds virginica. Let's also just take a quick look at the actual `Ttrain` targets to confirm our unique classes are all there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [2],\n",
       "       [2],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [2],\n",
       "       [0],\n",
       "       [1],\n",
       "       [2],\n",
       "       [1],\n",
       "       [2],\n",
       "       [2],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicator Variables, Numpy, and Broadcasting \n",
    "Here comes the magical part of the lab. How do we  convert our target array `Ttrain` into indicator variables? This will require a new Numpy concept called broadcasting. Broadcasting is typically an implicit concept used by Numpy to allow array arithmetic between arrays of different shapes or sizes. Below are some quotes that help convey the essence of broadcasting.\n",
    "\n",
    "> The term broadcasting describes how numpy treats arrays with different shapes during arithmetic operations. Subject to certain constraints, the smaller array is “broadcast” across the larger array so that they have compatible shapes.\n",
    "\n",
    "> In the context of deep learning, we also use some less conventional notation. We allow the addition of matrix and a vector, yielding another matrix: $C = A + b$, where $ C_{i,j} = A_{i,j} + b_j$. In other words, the vector $b$ is added to each row of the matrix. **This shorthand eliminates the need to define a matrix with $b$ copied into each row before doing the addition. This implicit copying of $b$ to many locations is called broadcasting.**\n",
    "\n",
    "\n",
    "Broadcasting essentially helps save us time and memory. Going in-depth into broadcasting is out of scope for this lab, but if you wish to learn more check out the references below.\n",
    "\n",
    "**References:**\n",
    "- [Gentle Introduction to Broadcasting](https://machinelearningmastery.com/broadcasting-with-numpy-arrays)\n",
    "- [Numpy Broadcasting Docs](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind the following code that checks if each element meets a desired condition. In the case below our condition is that we are checking which elements in `Ttrain` equal to 1. Remember that this line of code outputs a boolean array where the true values correspond to elements in `Ttrain` that met our condition and the false values correspond to elements in `Ttrain` that failed our condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take this aforementioned idea of checking if each element meets a desired condition and apply it to all our unique  `Ttrain` targets at once by using broadcasting. Meaning, with a single line of code we want to generate three boolean arrays, like the one above, but now instead of simply checking which elements are equal to 1, we want to check which elements are equal to 0, 1, or 2, i.e. our unique class targets.\n",
    "\n",
    "We can do so using the below code which essentially converts ours targets into indicator variables. The below code works as if we took the boolean arrays for `Ttrain == 0`, `Ttrain == 1`, and `Ttrain == 2` and concatenated them into a single array. However, now it only takes us one line of code and works for any number of classes! Most of the complicated work here is being done behind the scenes with broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [ True, False, False],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [False, False,  True],\n",
       "       [False, False,  True],\n",
       "       [False,  True, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False],\n",
       "       [ True, False, False]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ttrain == np.unique(Ttrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind what is happening is that we have two Numpy arrays, `Ttrain` and `np.unique(Ttrain)`, being compared to create a boolean array. Recall the shape of `Ttrain` is (120, 1) and the shape of `np.unique(Ttrain)` is (3,). A note about broadcasting to keep in mind is that it can only be performed when the shape of each dimension in the arrays are equal or when one of the arrays has the dimension size of 1. Here we can see our `Ttrain` has size 1 in the 2nd dimension (column dimension). This means that `np.unique(Ttrain)` can be broadcasted with `Ttrain`. Thus, this creates our output array above which has a shape of (120, 3).\n",
    "\n",
    "The results of this broadcasting automatically creates a boolean version of our desired indicator variables. The final step we'll need to take is converting our boolean array to 1s and 0s. We can do this by simply recasting the boolean array as an integer using Numpy's  `astype()` method. This works because the integer 1 is associated with true and the integer 0 is associated with false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [1, 0, 0],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [0, 0, 1],\n",
       "       [0, 0, 1],\n",
       "       [0, 1, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0],\n",
       "       [1, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(Ttrain == np.unique(Ttrain)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welp we did it! We somehow managed to convert our `Ttrain` targets into indicator variables through Numpy's magical broadcasting abilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "Now its time for you to try. Using the idea of broadcasting presented above finish the `convert_to_indicators()` function for converting `Ttrain` and `Ttest` into indicator variables.\n",
    "\n",
    "1. Convert `Ttrain` into an indicator variable and store the output into `Titrain`\n",
    "2. Convert `Ttest` into an indicator variable and store the output into `Titest`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_indicators():\n",
    "    global Titrain, Titest\n",
    "    # TODO (1)\n",
    "    Titrain = \n",
    "    # TODO (2)\n",
    "    Titest = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test if your `convert_to_indicators()` function works, run the following code. The `assert` code checks if your code produces the correct results and throws an error if it doesn't. **If you are failing the `assert` tests then something most likely went wrong in your function (check your shapes)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train indicator shape: (120, 3)\n",
      "Test indicator shape: (30, 3)\n"
     ]
    }
   ],
   "source": [
    "convert_to_indicators()\n",
    "print(\"Train indicator shape: {}\".format(Titrain.shape))\n",
    "print(\"Test indicator shape: {}\".format(Titest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (np.argmax(Titrain, axis=1) == Ttrain.flatten()).all()\n",
    "assert (np.argmax(Titest, axis=1) == Ttest.flatten()).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `Titrain` and `Titest` in our logistic regression algorithm!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Training\n",
    "Now its time to implement our logistic regression algorithm. \n",
    "\n",
    "<img src=\"http://webpages.uncc.edu/mlee173/teach/itcs4156/images/class/linearlogreg.png\" width=450 />\n",
    "\n",
    "Remember, the logistic regression gradient update is as follows.\n",
    "$$\n",
    "\\wv \\leftarrow \\wv + \\alpha \\Xm^\\top \\Big(  \\Tm - g(\\Xm)\\Big).\n",
    "$$\n",
    "\n",
    "Where $w$ represents our weights, $X^T$ represents our data transformed, $T$ represents our targets, $g(X)$ represents our predictions, i.e. $X \\cdot w$, and $\\alpha$ represents our learning rate. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in order to establish the probability of a given data sample belonging to one of our three flower classes, i.e. ${P(T = k | x)}$, we need to compute the softmax. Let's implement the softmax function before getting started.\n",
    "\n",
    "### TODO\n",
    "\n",
    "Implement the `softmax()` function and its wrapper function `g()`. Recall that the softmax is equal to the following formula.\n",
    "\n",
    "$$\n",
    " softmax = {P(T = k | x)} = \\frac{e^{z_k}}{\\sum_{i=1}^K e^{z_i}}\n",
    "$$\n",
    "\n",
    "Here $z$ represents our predictions made by computing $X \\cdot w$ which is calculated by the function `g()`.\n",
    "\n",
    "1. Using Numpy's `np.exp()` function compute the numerator of the softmax function, i.e. apply the exponential function to all our predictions `z`.\n",
    "2. Compute the denominator of the softmax function, i.e. apply the exponential function to all our predictions `z` and sum them.\n",
    "    1. Hint: Try using Numpy's `np.sum()`\n",
    "    2. Hint: If you're getting broadcasting errors make sure the numerator and denominator have the same shapes!\n",
    "3. Return the softmax output calced inside `softmax()` function.\n",
    "    1. Hint: Simply divide the numerator by the denominator\n",
    "4. Call the `softmax()` function and pass as its parameter the predictions calculated from dotting `X` and `w`. In addition, return the output of the `softmax()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    if not isinstance(z, np.ndarray):\n",
    "        z = np.asarray(z)\n",
    "    # TODO (1)\n",
    "    numerator = \n",
    "    # TODO (2)\n",
    "    denominator = \n",
    "    # TODO (3)\n",
    "    return\n",
    "\n",
    "# Wrapper for softmax\n",
    "def g(X, w):\n",
    "    # TODO (4)\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the logistic regression algorithm below. We have already created a new variable with the bias already added called `X1train`. Be sure to use `X1train` when computing the predictions and weight updates!\n",
    "\n",
    "### TODO\n",
    "\n",
    "1. Compute the class probabilities by calling the `g()` function which takes in the current weights `w` and all our training data with bias added `X1train`. Store the predicted class probabilities in `ys`.\n",
    "2. Compute the logistic regression weight update using the predicted class probabilities `ys`, the train data with bias `X1train`, and the training indicator varaibles `Titrain`.\n",
    "    1. Hint: If you are getting an error \"overflow encountered in exp\" simply try rerunning the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.91806145e-133 1.37747108e-058 1.00000000e+000]\n",
      " [8.16501208e-048 1.00000000e+000 1.52746594e-050]\n",
      " [1.00000000e+000 6.95361367e-074 0.00000000e+000]\n",
      " [1.00000000e+000 1.33563409e-071 0.00000000e+000]\n",
      " [2.44192608e-130 1.00143377e-065 1.00000000e+000]\n",
      " [5.52451162e-173 8.47681028e-110 1.00000000e+000]\n",
      " [4.01150045e-112 1.19186790e-052 1.00000000e+000]\n",
      " [4.74196015e-144 9.93958010e-076 1.00000000e+000]\n",
      " [1.00000000e+000 1.89206845e-074 0.00000000e+000]\n",
      " [2.32653736e-042 1.00000000e+000 3.19267625e-062]\n",
      " [1.00000000e+000 6.59536618e-057 0.00000000e+000]\n",
      " [1.00000000e+000 1.32716553e-045 0.00000000e+000]\n",
      " [1.58259941e-149 1.07827519e-072 1.00000000e+000]\n",
      " [1.00000000e+000 4.10267396e-062 0.00000000e+000]\n",
      " [5.68598325e-100 7.21934388e-020 1.00000000e+000]\n",
      " [1.00000000e+000 7.96317732e-078 0.00000000e+000]\n",
      " [1.00000000e+000 1.81960589e-051 0.00000000e+000]\n",
      " [4.14725187e-036 1.00000000e+000 1.20060323e-031]\n",
      " [1.47605248e-036 1.00000000e+000 9.89896229e-050]\n",
      " [2.71235549e-216 1.70624914e-102 1.00000000e+000]\n",
      " [5.06696901e-041 1.00000000e+000 5.03577937e-063]\n",
      " [1.00000000e+000 3.07181572e-049 0.00000000e+000]\n",
      " [7.49843115e-031 1.00000000e+000 2.68982856e-078]\n",
      " [1.00000000e+000 7.51039338e-051 0.00000000e+000]\n",
      " [2.07439256e-051 1.00000000e+000 1.01990651e-038]\n",
      " [6.18211318e-172 1.80483245e-073 1.00000000e+000]\n",
      " [1.00000000e+000 1.00075182e-085 0.00000000e+000]\n",
      " [1.11622357e-156 1.96393103e-069 1.00000000e+000]\n",
      " [2.78539276e-044 1.00000000e+000 6.15362074e-035]\n",
      " [1.00000000e+000 2.17475522e-073 0.00000000e+000]\n",
      " [1.00000000e+000 7.56858430e-094 0.00000000e+000]\n",
      " [6.77896605e-034 1.00000000e+000 7.35629875e-057]\n",
      " [1.00000000e+000 5.05578392e-088 0.00000000e+000]\n",
      " [1.00000000e+000 4.19437599e-063 0.00000000e+000]\n",
      " [2.25317606e-038 1.00000000e+000 1.11784721e-075]\n",
      " [4.32044722e-056 1.00000000e+000 1.32382297e-073]\n",
      " [1.00000000e+000 1.48097779e-075 0.00000000e+000]\n",
      " [2.31853085e-275 4.00533827e-149 1.00000000e+000]\n",
      " [1.10676266e-124 2.13888239e-069 1.00000000e+000]\n",
      " [1.17847157e-121 3.08556339e-043 1.00000000e+000]\n",
      " [1.66632202e-082 1.10905522e-029 1.00000000e+000]\n",
      " [3.13679087e-027 1.00000000e+000 1.09311524e-076]\n",
      " [6.74124224e-058 1.00000000e+000 6.98422284e-047]\n",
      " [7.88869663e-032 1.00000000e+000 2.05040885e-045]\n",
      " [2.41270846e-034 1.00000000e+000 6.06526137e-075]\n",
      " [3.20154772e-043 1.00000000e+000 2.75144465e-032]\n",
      " [1.00000000e+000 9.09757277e-070 0.00000000e+000]\n",
      " [1.00000000e+000 5.78738004e-095 0.00000000e+000]\n",
      " [1.05006361e-180 1.79354562e-101 1.00000000e+000]\n",
      " [5.16926308e-153 2.21508831e-071 1.00000000e+000]\n",
      " [1.00000000e+000 6.15164055e-030 3.93972692e-293]\n",
      " [1.00000000e+000 1.44514531e-062 0.00000000e+000]\n",
      " [5.34868132e-041 9.99921244e-001 7.87556694e-005]\n",
      " [1.00000000e+000 4.23544897e-065 0.00000000e+000]\n",
      " [1.16832445e-155 1.07116102e-098 1.00000000e+000]\n",
      " [6.02216888e-070 2.52067622e-021 1.00000000e+000]\n",
      " [8.28987997e-080 8.41218553e-014 1.00000000e+000]\n",
      " [1.00000000e+000 7.18166820e-074 0.00000000e+000]\n",
      " [6.76701124e-118 2.46895578e-051 1.00000000e+000]\n",
      " [3.47201184e-156 2.40675975e-088 1.00000000e+000]\n",
      " [4.02346270e-051 1.00000000e+000 3.96440917e-020]\n",
      " [1.52510206e-033 1.00000000e+000 7.81932633e-080]\n",
      " [4.74196015e-144 9.93958010e-076 1.00000000e+000]\n",
      " [1.38643036e-017 1.00000000e+000 3.86819266e-088]\n",
      " [8.10843454e-097 2.81766893e-028 1.00000000e+000]\n",
      " [1.00000000e+000 3.80575384e-066 0.00000000e+000]\n",
      " [1.14139288e-127 4.54322452e-070 1.00000000e+000]\n",
      " [1.00000000e+000 5.14461385e-059 0.00000000e+000]\n",
      " [1.11694389e-108 7.35443603e-051 1.00000000e+000]\n",
      " [3.45132844e-039 1.00000000e+000 2.59005324e-076]\n",
      " [1.00000000e+000 4.50123099e-070 0.00000000e+000]\n",
      " [1.00000000e+000 4.25588458e-073 0.00000000e+000]\n",
      " [6.57389993e-120 1.00335509e-049 1.00000000e+000]\n",
      " [3.91128341e-014 1.00000000e+000 4.57215942e-108]\n",
      " [1.00000000e+000 4.62268205e-060 0.00000000e+000]\n",
      " [1.00000000e+000 7.75396192e-088 0.00000000e+000]\n",
      " [1.00000000e+000 2.93705612e-054 0.00000000e+000]\n",
      " [1.00000000e+000 3.20497290e-048 0.00000000e+000]\n",
      " [1.21251046e-155 1.57104661e-078 1.00000000e+000]\n",
      " [9.89786010e-132 1.28498964e-080 1.00000000e+000]\n",
      " [1.00000000e+000 2.44968014e-052 0.00000000e+000]\n",
      " [2.69934934e-182 4.01127442e-104 1.00000000e+000]\n",
      " [2.00958820e-141 8.71642207e-074 1.00000000e+000]\n",
      " [1.80587865e-120 6.20750572e-048 1.00000000e+000]\n",
      " [1.00000000e+000 7.52518240e-066 0.00000000e+000]\n",
      " [6.86661310e-064 1.00000000e+000 2.73113235e-039]\n",
      " [1.00000000e+000 7.29535200e-065 0.00000000e+000]\n",
      " [1.69779670e-051 1.00000000e+000 4.17595591e-027]\n",
      " [1.00000000e+000 1.59585169e-074 0.00000000e+000]\n",
      " [4.65983672e-087 1.67855866e-024 1.00000000e+000]\n",
      " [3.72925777e-111 6.88406259e-042 1.00000000e+000]\n",
      " [1.00000000e+000 4.51971801e-062 0.00000000e+000]\n",
      " [1.91582828e-044 1.00000000e+000 4.77486722e-032]\n",
      " [1.12497380e-153 9.33797734e-089 1.00000000e+000]\n",
      " [2.85768736e-101 6.57187276e-041 1.00000000e+000]\n",
      " [5.08564402e-036 1.00000000e+000 1.21110463e-045]\n",
      " [6.82379136e-056 1.00000000e+000 2.16284259e-010]\n",
      " [2.55751133e-026 1.00000000e+000 5.38318817e-035]\n",
      " [1.00000000e+000 7.34310945e-053 0.00000000e+000]\n",
      " [1.00000000e+000 8.80498114e-073 0.00000000e+000]\n",
      " [1.00000000e+000 3.70732194e-073 0.00000000e+000]\n",
      " [7.95899472e-161 1.11533126e-088 1.00000000e+000]\n",
      " [1.00000000e+000 5.45441217e-052 0.00000000e+000]\n",
      " [7.76138826e-042 1.00000000e+000 1.16679065e-063]\n",
      " [6.08399315e-070 9.99998813e-001 1.18656793e-006]\n",
      " [3.76965368e-075 4.49488835e-025 1.00000000e+000]\n",
      " [3.13278759e-080 1.52855209e-009 9.99999998e-001]\n",
      " [2.06842466e-060 5.10961650e-019 1.00000000e+000]\n",
      " [2.50102098e-029 1.00000000e+000 4.88335648e-052]\n",
      " [3.83325147e-101 1.69324063e-038 1.00000000e+000]\n",
      " [1.00000000e+000 4.33193705e-063 0.00000000e+000]\n",
      " [4.27369900e-041 1.00000000e+000 3.94166239e-074]\n",
      " [5.64285375e-203 8.81715831e-100 1.00000000e+000]\n",
      " [7.45700282e-050 1.00000000e+000 1.48703298e-048]\n",
      " [5.68336192e-074 4.16293625e-018 1.00000000e+000]\n",
      " [1.64752223e-075 8.37271710e-024 1.00000000e+000]\n",
      " [3.68142406e-039 1.00000000e+000 2.90279633e-065]\n",
      " [1.00000000e+000 1.16825594e-063 0.00000000e+000]\n",
      " [1.00000000e+000 9.53091903e-061 0.00000000e+000]\n",
      " [1.00000000e+000 1.55152146e-066 0.00000000e+000]]\n",
      "[[4.41617529e-169 2.56315663e-095 1.00000000e+000]\n",
      " [1.99153559e-048 1.00000000e+000 2.36599908e-063]\n",
      " [6.46377945e-048 1.13285949e-007 9.99999887e-001]\n",
      " [1.01918283e-032 1.00000000e+000 3.71859982e-057]\n",
      " [1.66782674e-159 4.09886582e-096 1.00000000e+000]\n",
      " [1.00000000e+000 9.29817838e-052 0.00000000e+000]\n",
      " [1.00000000e+000 1.10457838e-066 0.00000000e+000]\n",
      " [1.09927488e-043 1.00000000e+000 1.64066270e-053]\n",
      " [2.43170634e-055 1.00000000e+000 2.08935288e-031]\n",
      " [2.87644222e-026 1.00000000e+000 1.03709477e-112]\n",
      " [9.08263542e-215 3.18738263e-142 1.00000000e+000]\n",
      " [1.00000000e+000 4.45439635e-052 0.00000000e+000]\n",
      " [1.35323123e-039 1.00000000e+000 8.01236388e-078]\n",
      " [1.99971503e-040 1.00000000e+000 1.34262668e-081]\n",
      " [1.00000000e+000 6.73081668e-055 0.00000000e+000]\n",
      " [4.31011996e-159 1.88450708e-063 1.00000000e+000]\n",
      " [7.72817446e-049 1.00000000e+000 1.02142858e-017]\n",
      " [2.92258870e-053 1.00000000e+000 1.94960595e-014]\n",
      " [2.05404997e-173 7.09091143e-094 1.00000000e+000]\n",
      " [1.00000000e+000 1.20707736e-060 0.00000000e+000]\n",
      " [1.00000000e+000 2.69869445e-052 0.00000000e+000]\n",
      " [8.83650910e-035 1.00000000e+000 4.05336693e-085]\n",
      " [1.19613994e-033 1.00000000e+000 1.33460395e-026]\n",
      " [2.08984351e-130 7.23062905e-046 1.00000000e+000]\n",
      " [1.08073388e-132 1.87712109e-074 1.00000000e+000]\n",
      " [2.99126149e-125 6.09182713e-056 1.00000000e+000]\n",
      " [1.00404726e-179 2.89524843e-082 1.00000000e+000]\n",
      " [5.93656682e-154 4.20477248e-065 1.00000000e+000]\n",
      " [1.00000000e+000 1.08287555e-099 0.00000000e+000]\n",
      " [1.16070388e-044 1.00000000e+000 5.16550603e-023]]\n"
     ]
    }
   ],
   "source": [
    "# Convert to indicators\n",
    "convert_to_indicators()\n",
    "\n",
    "# Set shape parameters\n",
    "Ntrain, D = Xtrain.shape\n",
    "Ntest = Xtest.shape[0]\n",
    "K = Titrain.shape[1]\n",
    "\n",
    "# initialize the weight matrix\n",
    "w = np.random.rand(D+1, K)\n",
    "\n",
    "# iterate to update weights\n",
    "niter = 1000\n",
    "alpha = 0.1\n",
    "\n",
    "# Add bias to data\n",
    "X1train = np.hstack((np.ones((Ntrain, 1)), Xtrain))\n",
    "\n",
    "likeli = []\n",
    "for step in range(niter):\n",
    "    # TODO (1)\n",
    "    ys =\n",
    "    # TODO (2)\n",
    "    w = \n",
    "    \n",
    "# Get class probabilities for each sample in X1train\n",
    "Ytrain = g(X1train, w)\n",
    "\n",
    "# Add bias to test data\n",
    "X1test = np.hstack((np.ones((Ntest,1)), Xtest))\n",
    "# Get class probabilities for each sample in X1test\n",
    "Ytest = g(X1test, w)\n",
    "\n",
    "print(Ytrain)\n",
    "print(Ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the logistic regression algorithm we should now have the predicted class probabilities for the training data `Ytrain`, and testing data `Ytest`. The question now is how do we get our predicted labels?\n",
    "\n",
    "The answer is quite simple! Recall, that there are three probabilities for each data sample (as seen in the above output). Logically, we want to label each data sample based on the class with the highest probability. We can do so by taking the argmax for each row in our data. The idea behind argmax is to return the column index that contains the highest probability.  Argmax works great here because our column indices are synonymous with our class targets because our targets are formatted as indicator variables!\n",
    "\n",
    "### TODO:\n",
    "Compute the argmax over our class probabilities for `Ytrain` and `Ytest`.\n",
    "\n",
    "1. Take the argmax of the training class probabilities `Ytrian` using Numpy's `argmax()` function. Store the predicted labels in `Ltrain`.\n",
    "    1. Hint: Remember to take the `argmax()` over the rows!\n",
    "2. Take the argmax of the testing class probabilities `Ytest` using Numpy's `argmax()` function. Store the predicted labels in `Ltest`.\n",
    "    1. Hint: Remember to take the `argmax()` over the rows!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 2, 2, 2, 2, 0, 1, 0, 0, 2, 0, 2, 0, 0, 1, 1, 2, 1, 0,\n",
       "       1, 0, 1, 2, 0, 2, 1, 0, 0, 1, 0, 0, 1, 1, 0, 2, 2, 2, 2, 1, 1, 1,\n",
       "       1, 1, 0, 0, 2, 2, 0, 0, 1, 0, 2, 2, 2, 0, 2, 2, 1, 1, 2, 1, 2, 0,\n",
       "       2, 0, 2, 1, 0, 0, 2, 1, 0, 0, 0, 0, 2, 2, 0, 2, 2, 2, 0, 1, 0, 1,\n",
       "       0, 2, 2, 0, 1, 2, 2, 1, 1, 1, 0, 0, 0, 2, 0, 1, 1, 2, 2, 2, 1, 2,\n",
       "       0, 1, 2, 1, 2, 2, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (1)\n",
    "Ltrain = \n",
    "Ltrain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 2, 1, 2, 0, 0, 1, 1, 1, 2, 0, 1, 1, 0, 2, 1, 1, 2, 0, 0, 1,\n",
       "       1, 2, 2, 2, 2, 2, 0, 1])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (2)\n",
    "Ltest =\n",
    "Ltest "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome, now we have our predicted labels which corresponds to the classes that our logistic algorithm thought were the most likely correct for all the training and testing data samples. Let's visualize these predictions and see how they did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training results')"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Ttrain, 'ro')\n",
    "plt.plot(Ltrain, 'bx')\n",
    "plt.title(\"Training results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Test results')"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(Ttest, 'ro')\n",
    "plt.plot(Ltest, 'bx')\n",
    "plt.title(\"Test results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code is another way to visualize which of our `Ttest` samples we got right (True) or wrong (False). Here we are simply comparing our predicted labels `Ltest` and our target labels `Ttest`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True, False,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ltest==Ttest.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's quantify our algorithms preformance by computing the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(y, t, name):\n",
    "    N = y.shape[0]\n",
    "    n_correct = np.sum(y.flat == t.flat)\n",
    "    n_correct_percent =  (n_correct / N) * 100\n",
    "    print(\"{} accyracy:\\t{}/{}\\t{} %\".format(name, n_correct, N, n_correct_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accyracy:\t117/120\t97.5 %\n",
      "Test accyracy:\t29/30\t96.66666666666667 %\n"
     ]
    }
   ],
   "source": [
    "print_accuracy(Ltrain, Ttrain, \"Train\")\n",
    "print_accuracy(Ltest, Ttest, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our logistic regression algorithm does pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying New Metrics\n",
    "\n",
    "## Confusion Matrix \n",
    "Now it's time to visualize our results in a slightly different yet very meaningful way using a confusion matrix! A confusion matrix allows us to see the number of samples we mislabeled and which classes the samples were mislabeled as. While this may sound simplistic it is actually very powerful!\n",
    "\n",
    "Although our confusion matrix will have a shape of (3, 3), since we have three classes, the ideas of a binary confusion matrix still apply, however things are slightly more muddled. It's important to remember that a confusion matrix comes down to a matter of perspective! Let's see what this means by looking at the rows and columns of a confusion matrix of arbitrary size. \n",
    "\n",
    "### Reading a NxN Confusion Matrix\n",
    "\n",
    "Recall from the lecture notes that for a confusion matrix each row represents the predicted labels made by the model and each column represents the actual targets. This means that the main diagonal contains the number of samples we predicted correctly, i.e. our **true positives** and **true negatives**. Meanwhile, any values off the main diagonal corresponds to misclassified data samples. \n",
    "\n",
    "Looking row wise we can observe the **true positives** and **false positives**. Like the binary case **true positives** lie on the main diagonal. However, the rest of the row wise elements correspond to **false positives**. These being values that were predicted to be the class corresponding to the given row, but actually belong to the class corresponding to the given column. \n",
    "\n",
    "Likewise, looking column wise we can observe the **true negatives** and **false negatives**. Like the binary case **true negatives** lie on the main diagonal. However, the rest of the column wise elements correspond to **false negatives**. These being values that belong to the class corresponding to the given column, but were predicted to be the class corresponding to the given row. As we can see this is the reverse of the row perspective! \n",
    "\n",
    "### References:\n",
    "- [What is true positive and true negative – confusion matrix](https://moredvikas.wordpress.com/2017/09/12/what-is-true-positive-and-true-negative-confusion-matrix/)\n",
    "- [Simple guide to confusion matrix terminology](https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)\n",
    "\n",
    "### TODO:\n",
    "1. Create a 3x3 confusion matrix for your test results by filling in the `confusion_matrix()` function. You can write your own or use the `confusion_matrix()` function from the [class notes](https://nbviewer.jupyter.org/url/webpages.uncc.edu/mlee173/teach/itcs4156/notebooks/notes/Note-Linear%20Classification.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(Y, T):\n",
    "    \"\"\"\n",
    "        Y    ndarray\n",
    "             predicted labels\n",
    "        T    ndarray\n",
    "             target labels\n",
    "             \n",
    "        @cfm DataFrame\n",
    "             confusion matrix\n",
    "    \"\"\"\n",
    "    # TODO (1): Add code here\n",
    "    \n",
    "    \n",
    "    return cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2\n",
       "0  7   0   0\n",
       "1  0  12   0\n",
       "2  0   1  10"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfm = confusion_matrix(Ltest.flat, Ttest.flat)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice that `cfm` is a Pandas DataFrmae not a Nump array!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision, Recall, and F1-score \n",
    "Now that we have our confusion matrix we can use it to compute new metrics that inform us about different aspects of our models performance. Recall that we discussed precision, recall, and the F1-score in the class notes. Let's do a quick review of what these scores mean and how to compute them.\n",
    "\n",
    "### Precision\n",
    "Precision represents the accuracy when only looking at our **positive predictions**. Meaning, we want to know how many of our positive predictions were correct out of all the positive predictions made. Hence, we divide by the sum of the true positives $\\mathrm{TP}$ and false positives $\\mathrm{FP}$. Notice, the sum these two variables is equal the total number of positive predictions made for a given class.\n",
    "$$\n",
    "Precision = \\mathrm{\\frac{TP}{TP + FP}}\n",
    "$$\n",
    "\n",
    "### Recall\n",
    "Recall represents the accuracy when only looking at the **positive targets**. This means we want to know how many positive predictions we got correct out of the actual positive targets. Hence, we divide by the sum of true positives $\\mathrm{TP}$ and false negatives $\\mathrm{FN}$. Notice the sum these two variables is equal the total number of targets for a given class.\n",
    "$$\n",
    "Recall = \\mathrm{\\frac{TP}{TP + FN}}\n",
    "$$\n",
    "\n",
    "### F1-score\n",
    "The F1-score is used to gain a combined measure of both precision and recall. To do so we calculate the harmonic mean (the reciprocal of the arithmetic mean) of the precision and recall scores. F1-score is a good alternative to accuracy if the number false negatives and false positives are important to your problem. \n",
    "$$\n",
    "F_1 = 2 \\times \\frac{Precision \\times Recall}{Precision+Recall} = \\frac{2 \\times tp}{2 \\times tp + fp + fn}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "Compute precision, recall, and F1-score using the elements confusion matrix `cfm`. Recall that the **rows** of the confusion matrix correspond to the *true positives* and *false positives*. Likewise, the **columns** correspond to *true negatives* and *false negatives*.\n",
    "\n",
    "**Remember that `cfm` is a Pandas DataFrmae not a Nump array! You must index `cfm` using `cfm.iloc()`!**\n",
    "\n",
    "1. Compute the percision score using our confusion matrix `cfm`\n",
    "2. Compute the recall score using our confusion matrix `cfm`\n",
    "3. Compute the F1-score score using the percision and recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setosa:\n",
      "\tPrecision (setosa): 1.00000\n",
      "\tRecall (setosa): 1.00000\n",
      "\tF1 Score (setosa): 1.00000\n",
      "versicolor:\n",
      "\tPrecision (versicolor): 1.00000\n",
      "\tRecall (versicolor): 0.92308\n",
      "\tF1 Score (versicolor): 0.96000\n",
      "virginica:\n",
      "\tPrecision (virginica): 0.90909\n",
      "\tRecall (virginica): 1.00000\n",
      "\tF1 Score (virginica): 0.95238\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    t_name = iris.target_names[i]\n",
    "    print(\"{}:\".format(t_name))\n",
    "    # TODO (1)\n",
    "    precision = \n",
    "    # TODO (2)\n",
    "    recall = \n",
    "    # TODO (3)\n",
    "    f1 =\n",
    "    # Print scores for each class\n",
    "    print(\"\\tPrecision ({}): {:.5f}\".format(t_name, precision))\n",
    "    print(\"\\tRecall ({}): {:.5f}\".format(t_name, recall))\n",
    "    print(\"\\tF1 Score ({}): {:.5f}\".format(t_name, f1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
